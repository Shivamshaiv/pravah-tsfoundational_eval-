# -*- coding: utf-8 -*-
"""BRPL-operational-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BjMH571Kq40jIEy4O-eXqke_wXijLP9h
"""


"""## V3"""

# === Imports ===
import pandas as pd
import numpy as np
from datetime import datetime
import pytz
from meteostat import Hourly
from darts import TimeSeries, concatenate
from darts.models import TiDEModel
from darts.metrics import mape, mae, rmse
from darts.dataprocessing.transformers import Scaler
from sklearn.preprocessing import MinMaxScaler
from pytorch_lightning.callbacks import EarlyStopping
import torch
import torch.nn as nn
import holidays
import warnings

warnings.filterwarnings("ignore")

torch.set_float32_matmul_precision('high')

# === Calendar Features ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add 15-minute specific features
    features["quarter_hour_sin"] = np.sin(2 * np.pi * index.minute / 60)
    features["quarter_hour_cos"] = np.cos(2 * np.pi * index.minute / 60)

    in_holidays = holidays.country_holidays("IN", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(in_holidays.keys()))).astype(int)
    return features

# === Load Electricity Demand (Keep at 15-minute frequency) ===
df = pd.read_csv("combined_demand_timeseries.csv")
df.columns = df.columns.str.strip()
df["datetime"] = pd.to_datetime(df["datetime"])
df.set_index("datetime", inplace=True)
df = df.sort_index()

# Ensure the demand data is timezone-naive (assuming it's already in IST)
if df.index.tz is not None:
    print(f"Converting demand data from {df.index.tz} to timezone-naive IST")
    # If it has timezone info, convert to IST then make naive
    df.index = df.index.tz_convert('Asia/Kolkata').tz_localize(None)
else:
    print("Demand data is already timezone-naive (assuming IST)")

# Ensure 15-minute frequency
df_15min = df.copy()
print(f"Demand data shape at 15-minute frequency: {df_15min.shape}")

# === Load Weather Data with Linear Interpolation ===
# Weather data from Meteostat - returns UTC data without timezone info
station_ids = ['42182']
start, end = df_15min.index.min(), df_15min.index.max()
weather_stations = {}

# Define IST timezone
ist = pytz.timezone('Asia/Kolkata')

for station_id in station_ids:
    try:
        print(f"Fetching weather data for station: {station_id}")

        # Meteostat expects timezone-naive datetime in UTC
        # Convert IST times to UTC for the API call
        # Ensure we have Timestamp objects
        start_timestamp = pd.Timestamp(start)
        end_timestamp = pd.Timestamp(end)

        # Since the demand data is timezone-naive (assumed IST), localize it first
        start_ist_aware = ist.localize(start_timestamp)
        end_ist_aware = ist.localize(end_timestamp)

        # Convert to UTC
        start_utc_aware = start_ist_aware.astimezone(pytz.UTC)
        end_utc_aware = end_ist_aware.astimezone(pytz.UTC)

        # Make timezone-naive for Meteostat API
        start_utc = start_utc_aware.tz_localize(None)
        end_utc = end_utc_aware.tz_localize(None)

        data = Hourly(station_id, start_utc, end_utc).fetch()

        if data.empty:
            print(f"No data returned for station {station_id}")
            continue

        data = data[["temp", "dwpt", "rhum", "wspd", "pres"]] #"prcp"

        # Convert the UTC index to IST
        # First, make it timezone-aware as UTC
        data.index = pd.to_datetime(data.index).tz_localize('UTC')
        # Then convert to IST
        data.index = data.index.tz_convert('Asia/Kolkata')
        # Finally, make it timezone-naive (but now in IST)
        data.index = data.index.tz_localize(None)

        print(f"Weather data converted from UTC to IST (timezone-naive)")
        print(f"Weather data shape: {data.shape}")

        # Fill missing values at hourly level using interpolation
        data = data.interpolate(method='linear', limit_direction='both')

        # Create 15-minute index aligned with demand data
        full_15min_index = df_15min.index

        # Reindex to 15-minute frequency and use linear interpolation
        data_15min = data.reindex(full_15min_index)
        data_15min = data_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values with backward fill (avoid forward looking)
        data_15min = data_15min.bfill()

        # Rename weather columns to include station ID
        data_15min.columns = [f"station_{station_id}_{col}" for col in data_15min.columns]

        weather_stations[station_id] = data_15min

        print(f"Station {station_id} - Weather features: {len(data_15min.columns)}")
        print(f"Missing values after interpolation: {data_15min.isnull().sum().sum()}")

    except Exception as e:
        print(f"Warning: Could not load data for station {station_id}. Error: {e}")
        import traceback
        traceback.print_exc()

# Combine all weather stations into one DataFrame
if weather_stations:
    weather_df = pd.concat(list(weather_stations.values()), axis=1)
    print(f"Total weather features: {weather_df.shape[1]}")
else:
    print("No weather data loaded. Creating empty weather DataFrame.")
    weather_df = pd.DataFrame(index=df_15min.index)

# === Create Future Covariates (Calendar + Weather Features) ===
calendar_df = create_calendar_features(df_15min.index)

# Combine calendar and weather features as future covariates
future_features = pd.concat([calendar_df, weather_df], axis=1)

# Add squared terms for weather variables
weather_cols = [col for col in weather_df.columns]
for col in weather_cols:
    if 'temp' in col:
        future_features[f"{col}_squared"] = weather_df[col] ** 2

print(f"Total future covariate features: {future_features.shape[1]}")
print("Future covariates include:")
print(f"- Calendar features: {calendar_df.shape[1]}")
print(f"- Weather features: {weather_df.shape[1]}")
print(f"- Temperature squared features: {future_features.shape[1] - calendar_df.shape[1] - weather_df.shape[1]}")

# === Create Darts TimeSeries ===
# Ensure proper 15-minute frequency and interpolate any missing values
# Create a complete 15-minute index
freq_15min = pd.date_range(start=df_15min.index.min(),
                          end=df_15min.index.max(),
                          freq='15min')

# Reindex and interpolate the target data
df_15min = df_15min.reindex(freq_15min)
df_15min = df_15min.interpolate(method='linear', limit_direction='both')

# Reindex and interpolate the future features
future_features = future_features.reindex(freq_15min)
future_features = future_features.interpolate(method='linear', limit_direction='both')

# Handle any remaining NaN values
df_15min = df_15min.fillna(method='bfill').fillna(method='ffill')
future_features = future_features.fillna(method='bfill').fillna(method='ffill')

print(f"Data reindexed to 15-minute frequency")
print(f"Target missing values after interpolation: {df_15min.isnull().sum().sum()}")
print(f"Future features missing values after interpolation: {future_features.isnull().sum().sum()}")

target_ts = TimeSeries.from_dataframe(df_15min, value_cols=["demand"], freq='15min')
future_covariates_ts = TimeSeries.from_dataframe(future_features, freq='15min')

# Note: No past covariates since everything is treated as future covariates
print("No past covariates - all weather data treated as future covariates")

# === Train/Validation/Test Split ===
train_start = pd.Timestamp("2017-01-01 06:45:00")
train_end = pd.Timestamp("2024-01-01 07:00:00")
val_start = pd.Timestamp("2024-01-01 06:45:00")
val_end = pd.Timestamp("2025-01-01 07:00:00")
test_start = pd.Timestamp("2025-01-01 06:45:00")

# Target series splits
train_ts = target_ts.drop_before(train_start).drop_after(train_end)
val_ts = target_ts.drop_before(val_start).drop_after(val_end)
test_ts = target_ts.drop_before(test_start)

# Future covariates splits (calendar + weather features)
train_future_cov = future_covariates_ts.drop_before(train_start).drop_after(train_end)
val_future_cov = future_covariates_ts.drop_before(val_start).drop_after(val_end)
test_future_cov = future_covariates_ts.drop_before(test_start)

# Verification
print(f"Training data starts at: {train_ts.start_time()}")
print(f"Training data ends at: {train_ts.end_time()}")
print(f"Validation data starts at: {val_ts.start_time()}")
print(f"Validation data ends at: {val_ts.end_time()}")
print(f"Test data starts at: {test_ts.start_time()}")
print(f"Test data ends at: {test_ts.end_time()}")

# === Scale Target and Covariates ===
scaler_y = Scaler(scaler=MinMaxScaler())
scaler_future_cov = Scaler(scaler=MinMaxScaler())

# Scale target
train_y = scaler_y.fit_transform(train_ts).astype(np.float32)
val_y = scaler_y.transform(val_ts).astype(np.float32)
test_y = scaler_y.transform(test_ts).astype(np.float32)

# Scale future covariates (calendar + weather)
train_future_cov = scaler_future_cov.fit_transform(train_future_cov).astype(np.float32)
val_future_cov = scaler_future_cov.transform(val_future_cov).astype(np.float32)
test_future_cov = scaler_future_cov.transform(test_future_cov).astype(np.float32)

# === Configure TiDEModel ===
model = TiDEModel(
    input_chunk_length=192,      # 31 hours of input
    output_chunk_length=164,     # ~41 hours of output
    hidden_size=1024,
    temporal_decoder_hidden=32,
    temporal_width_past=4,
    temporal_width_future=4,
    num_encoder_layers=5,
    num_decoder_layers=5,
    decoder_output_dim=32,
    use_layer_norm=False,
    dropout=0.2,
    n_epochs=50,
    batch_size=64,
    optimizer_kwargs={"lr": 5e-4}, #1e-3
    # Using default MSE loss instead of custom loss
    pl_trainer_kwargs={
        "accelerator": "gpu",
        "devices": 1,
        "precision": "32-true",
        "gradient_clip_val": 0.5,
        "callbacks": [EarlyStopping(monitor="val_loss", patience=10)]
    },
    save_checkpoints=True,
    force_reset=True,
    random_state=42
)

# === Train the Model ===
print("Training model with all features as future covariates...")
print(f"Input chunk length: {model.input_chunk_length} steps ({model.input_chunk_length * 15} minutes)")
print(f"Output chunk length: {model.output_chunk_length} steps ({model.output_chunk_length * 15} minutes)")
print(f"Future covariates: {future_features.shape[1]} features")

model.fit(
    series=train_y,
    future_covariates=train_future_cov,
    val_series=val_y,
    val_future_covariates=val_future_cov,
    verbose=True
)

# === Load Best Model and Forecast ===
model = model.load_from_checkpoint(model.model_name, best=True)

forecast_list = model.historical_forecasts(
    series=test_y,
    future_covariates=test_future_cov,
    start=test_y.start_time(),
    forecast_horizon=164,
    stride=96,
    retrain=False,
    verbose=True,
    last_points_only=False
)

# === Extract and Evaluate Last 24 Hours ===
forecast_24 = [forecast[-96:] for forecast in forecast_list]
forecast_combined = concatenate(forecast_24)
actual_combined = test_y.slice(forecast_combined.start_time(), forecast_combined.end_time())
forecast_unscaled = scaler_y.inverse_transform(forecast_combined)
actual_unscaled = scaler_y.inverse_transform(actual_combined)

print("\n=== Evaluation Metrics ===")
print("MAE:", mae(actual_unscaled, forecast_unscaled))
print("MAPE:", mape(actual_unscaled, forecast_unscaled))
print("RMSE:", rmse(actual_unscaled, forecast_unscaled))

"""## End of V3"""

# === Imports ===
import pandas as pd
import numpy as np
from datetime import datetime
from meteostat import Hourly
from darts import TimeSeries
from darts.models import TiDEModel
from darts.metrics import mape, mae, rmse
from darts.dataprocessing.transformers import Scaler
from sklearn.preprocessing import MinMaxScaler
from pytorch_lightning.callbacks import EarlyStopping
import torch
import torch.nn as nn
import holidays
import warnings
import os

warnings.filterwarnings("ignore")

# === Custom Loss Function ===
class CustomLast24hMAELoss(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, y_pred, y_true):
        # For 15-minute data, last 24h = 96 time steps (24 * 4)
        return torch.mean(torch.abs(y_true[:, -96:, :] - y_pred[:, -96:, :]))

# === Calendar Features ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add 15-minute specific features
    features["quarter_hour_sin"] = np.sin(2 * np.pi * index.minute / 60)
    features["quarter_hour_cos"] = np.cos(2 * np.pi * index.minute / 60)

    # Indian holidays
    in_holidays = holidays.country_holidays("IN", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(in_holidays.keys()))).astype(int)
    return features

# === Process Real-time Data ===
def process_realtime_data(file_path):
    print(f"Processing real-time data from {file_path}...")

    # Check if file exists
    if not os.path.exists(file_path):
        print(f"Warning: {file_path} not found. Returning empty DataFrame.")
        return pd.DataFrame()

    # Read the txt file
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()

    # Clean the Data_Time column by stripping leading/trailing whitespace
    df['Data_Time'] = df['Data_Time'].str.strip()

    # Parse the Data_Time column and create datetime
    df['datetime'] = pd.to_datetime(df['Data_Time'], format='%d-%b-%Y %I:%M:%S %p')

    # Use Load column as value
    df['value'] = df['Load']

    # Keep only datetime and value
    df = df[['datetime', 'value']].copy()
    df = df.sort_values('datetime').reset_index(drop=True)

    print(f"Real-time data: {len(df)} records from {df['datetime'].min()} to {df['datetime'].max()}")

    # Create complete 15-minute range for interpolation
    if len(df) > 0:
        full_range = pd.date_range(
            start=df['datetime'].min(),
            end=df['datetime'].max(),
            freq='15T'
        )

        # Create complete dataframe and merge
        full_df = pd.DataFrame({'datetime': full_range})
        merged_df = pd.merge(full_df, df, on='datetime', how='left')

        # Interpolate missing values
        merged_df['value'] = merged_df['value'].interpolate(method='linear')

        # Handle any remaining NaN values
        merged_df['value'] = merged_df['value'].bfill().ffill()

        missing_count = merged_df['value'].isna().sum()
        print(f"Real-time data after interpolation: {len(merged_df)} records, {missing_count} missing values")

        return merged_df[['datetime', 'value']]

    return df

# === Load Electricity Demand Data ===
print("Loading electricity demand data...")
df = pd.read_csv("/content/combined_time_series.csv")
df.columns = df.columns.str.strip()
df["datetime"] = pd.to_datetime(df["datetime"])
df.set_index("datetime", inplace=True)
df = df.sort_index()

# Trim data to 2025-07-01 10:00:00
cutoff_time = pd.Timestamp("2025-07-01 10:00:00")
df_trimmed = df.loc[df.index <= cutoff_time].copy()
print(f"Historical data trimmed to: {df_trimmed.index.max()}")
print(f"Historical data shape: {df_trimmed.shape}")

# === Load and Process Real-time Data ===
realtime_df = process_realtime_data("/content/brpl_data.txt")

# Combine historical and real-time data
if len(realtime_df) > 0:
    # Set datetime as index for real-time data
    realtime_df.set_index('datetime', inplace=True)

    # Filter real-time data to only include new data after cutoff
    realtime_df = realtime_df.loc[realtime_df.index > cutoff_time]

    if len(realtime_df) > 0:
        print(f"Adding real-time data: {len(realtime_df)} records from {realtime_df.index.min()} to {realtime_df.index.max()}")

        # Combine historical and real-time data
        df_combined = pd.concat([df_trimmed, realtime_df])
        df_combined = df_combined.sort_index()

        # Remove any duplicates
        df_combined = df_combined[~df_combined.index.duplicated(keep='last')]

        print(f"Combined data shape: {df_combined.shape}")
        print(f"Combined data range: {df_combined.index.min()} to {df_combined.index.max()}")
    else:
        print("No new real-time data to add")
        df_combined = df_trimmed
else:
    print("No real-time data found, using only historical data")
    df_combined = df_trimmed

# Keep at 15-minute frequency
df_15min = df_combined.copy()
print(f"Final demand data shape at 15-minute frequency: {df_15min.shape}")
print(f"Final data range: {df_15min.index.min()} to {df_15min.index.max()}")

# === Load Weather Data with Linear Interpolation ===
print("Loading and processing weather data...")
station_ids = ['42182']
start, end = df_15min.index.min(), df_15min.index.max()
weather_stations = {}

for station_id in station_ids:
    try:
        print(f"Fetching weather data for station: {station_id}")
        data = Hourly(station_id, start, end).fetch()
        data = data[["temp", "dwpt", "rhum", "wspd", "pres"]]

        # Fill missing values at hourly level using interpolation
        data = data.interpolate(method='linear', limit_direction='both')

        # Create 15-minute index aligned with demand data
        full_15min_index = df_15min.index

        # Reindex to 15-minute frequency and use linear interpolation
        data_15min = data.reindex(full_15min_index)
        data_15min = data_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values with backward fill
        data_15min = data_15min.bfill()

        # Rename weather columns to include station ID
        data_15min.columns = [f"station_{station_id}_{col}" for col in data_15min.columns]

        weather_stations[station_id] = data_15min

        print(f"Station {station_id} - Weather features: {len(data_15min.columns)}")
        print(f"Missing values after interpolation: {data_15min.isnull().sum().sum()}")

    except Exception as e:
        print(f"Warning: Could not load data for station {station_id}. Error: {e}")

# Combine all weather stations into one DataFrame
weather_df = pd.concat(list(weather_stations.values()), axis=1)
print(f"Total weather features: {weather_df.shape[1]}")

# === Create Future Covariates (Calendar + Weather Features) ===
print("Creating calendar and weather features...")
calendar_df = create_calendar_features(df_15min.index)

# Combine calendar and weather features as future covariates
future_features = pd.concat([calendar_df, weather_df], axis=1)

# Add weather interaction features
if len(station_ids) >= 1:
    station_temp = f"station_{station_ids[0]}_temp"
    if station_temp in future_features.columns:
        # Add squared terms for temperature
        future_features[f"{station_temp}_squared"] = weather_df[station_temp] ** 2

# Add more weather interactions
weather_cols = [col for col in weather_df.columns]
for col in weather_cols:
    if 'temp' in col:
        future_features[f"{col}_squared"] = weather_df[col] ** 2

print(f"Total future covariate features: {future_features.shape[1]}")
print("Future covariates include:")
print(f"- Calendar features: {calendar_df.shape[1]}")
print(f"- Weather features: {weather_df.shape[1]}")
print(f"- Weather interactions: {future_features.shape[1] - calendar_df.shape[1] - weather_df.shape[1]}")

# === Create Darts TimeSeries ===
print("Creating TimeSeries objects...")
target_ts = TimeSeries.from_dataframe(df_15min, value_cols=["value"])
future_covariates_ts = TimeSeries.from_dataframe(future_features)

print(f"Target series length: {len(target_ts)}")
print(f"Future covariates length: {len(future_covariates_ts)}")

# === Use All Data for Final Training ===
# Use 2016-2025 for training, 2024-2025 for validation (overlapping)
final_train_start = pd.Timestamp("2016-01-01 06:45:00")
final_train_end = df_15min.index.max()  # Use all available data for training
final_val_start = pd.Timestamp("2024-01-01 00:00:00")  # Validate on 2024-2025
final_val_end = df_15min.index.max()  # Use all data till the end

# Final training data (uses almost all available data)
final_train_ts = target_ts.drop_before(final_train_start).drop_after(final_train_end)
final_val_ts = target_ts.drop_before(final_val_start).drop_after(final_val_end)

# Future covariates for final training
final_train_future_cov = future_covariates_ts.drop_before(final_train_start).drop_after(final_train_end)
final_val_future_cov = future_covariates_ts.drop_before(final_val_start).drop_after(final_val_end)

# Verification
print(f"Final training data: {final_train_ts.start_time()} to {final_train_ts.end_time()}")
print(f"Final validation data: {final_val_ts.start_time()} to {final_val_ts.end_time()}")
print(f"Training samples: {len(final_train_ts)}")
print(f"Validation samples: {len(final_val_ts)}")

# === Scale Target and Covariates ===
print("Scaling data...")
scaler_y = Scaler(scaler=MinMaxScaler())
scaler_future_cov = Scaler(scaler=MinMaxScaler())

# Scale target
final_train_y = scaler_y.fit_transform(final_train_ts).astype(np.float32)
final_val_y = scaler_y.transform(final_val_ts).astype(np.float32)

# Scale future covariates
final_train_future_cov = scaler_future_cov.fit_transform(final_train_future_cov).astype(np.float32)
final_val_future_cov = scaler_future_cov.transform(final_val_future_cov).astype(np.float32)

torch.set_float32_matmul_precision('high')

# === Configure Final TiDEModel ===
print("Configuring final TiDE model...")
final_model = TiDEModel(
    input_chunk_length=124,      # 31 hours of input
    output_chunk_length=164,     # ~41 hours of output
    hidden_size=1024,
    temporal_decoder_hidden=32,
    temporal_width_past=4,
    temporal_width_future=4,
    num_encoder_layers=5,
    num_decoder_layers=5,
    decoder_output_dim=32,
    use_layer_norm=False,
    dropout=0.2,
    n_epochs=200,                 # Increased epochs for final model
    batch_size=64,
    optimizer_kwargs={"lr": 1e-3},
    loss_fn=CustomLast24hMAELoss(),
    pl_trainer_kwargs={
        "accelerator": "gpu",
        "devices": 1,
        "precision": "32-true",
        "gradient_clip_val": 1.0,
        "callbacks": [EarlyStopping(monitor="val_loss", patience=20)]  # Increased patience
    },
    save_checkpoints=True,
    force_reset=True,
    random_state=42
)

# === Train the Final Model ===
print("=" * 60)
print("TRAINING FINAL MODEL WITH COMPLETE DATASET")
print("=" * 60)
print(f"Training period: 2016-2025 (all available data)")
print(f"Validation period: 2024-2025")
print(f"Input chunk length: {final_model.input_chunk_length} steps ({final_model.input_chunk_length * 15} minutes)")
print(f"Output chunk length: {final_model.output_chunk_length} steps ({final_model.output_chunk_length * 15} minutes)")
print(f"Future covariates: {future_features.shape[1]} features")
print(f"Training samples: {len(final_train_y)}")
print(f"Total data used: {len(final_train_y) + len(final_val_y)} samples")

final_model.fit(
    series=final_train_y,
    future_covariates=final_train_future_cov,
    val_series=final_val_y,
    val_future_covariates=final_val_future_cov,
    verbose=True
)

print("=" * 60)
print("FINAL MODEL TRAINING COMPLETED!")
print("=" * 60)

# === Save the Final Model and Scalers ===
print("Saving final model and scalers...")

# Save the trained model
final_model.save("/content/final_tide_model.pkl")

# Save scalers for future use
import pickle
with open("/content/final_scaler_y.pkl", "wb") as f:
    pickle.dump(scaler_y, f)

with open("/content/final_scaler_future_cov.pkl", "wb") as f:
    pickle.dump(scaler_future_cov, f)

# Save the final combined dataset
df_15min.to_csv("/content/final_combined_timeseries.csv")

print("Final model and scalers saved successfully!")
print("Files saved:")
print("- /content/final_tide_model.pkl")
print("- /content/final_scaler_y.pkl")
print("- /content/final_scaler_future_cov.pkl")
print("- /content/final_combined_timeseries.csv")

# === Model Summary ===
print("\n" + "=" * 60)
print("FINAL MODEL SUMMARY")
print("=" * 60)
print(f"Model type: TiDEModel")
print(f"Training data: 2016-2025 ({len(final_train_y):,} samples)")
print(f"Validation data: 2024-2025 ({len(final_val_y):,} samples)")
print(f"Input length: {final_model.input_chunk_length} steps (31 hours)")
print(f"Output length: {final_model.output_chunk_length} steps (41 hours)")
print(f"Hidden size: {final_model.hidden_size}")
print(f"Encoder layers: {final_model.num_encoder_layers}")
print(f"Decoder layers: {final_model.num_decoder_layers}")
print(f"Future covariates: {future_features.shape[1]} features")
print(f"  - Calendar features: {calendar_df.shape[1]}")
print(f"  - Weather features: {weather_df.shape[1]}")
print(f"  - Interaction features: {future_features.shape[1] - calendar_df.shape[1] - weather_df.shape[1]}")
print(f"Real-time data included: {'Yes' if len(realtime_df) > 0 else 'No'}")
print("=" * 60)

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from meteostat import Hourly
from darts import TimeSeries
from darts.models import TiDEModel
from darts.dataprocessing.transformers import Scaler
import torch
import holidays
import pickle
import warnings
import os

warnings.filterwarnings("ignore")

# === Calendar Features Function ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add 15-minute specific features
    features["quarter_hour_sin"] = np.sin(2 * np.pi * index.minute / 60)
    features["quarter_hour_cos"] = np.cos(2 * np.pi * index.minute / 60)

    # Indian holidays
    in_holidays = holidays.country_holidays("IN", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(in_holidays.keys()))).astype(int)
    return features

# === Process Real-time Data ===
def process_realtime_data(file_path):
    print(f"Processing real-time data from {file_path}...")

    if not os.path.exists(file_path):
        print(f"Warning: {file_path} not found. Returning empty DataFrame.")
        return pd.DataFrame()

    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()
    df['Data_Time'] = df['Data_Time'].str.strip()
    df['datetime'] = pd.to_datetime(df['Data_Time'], format='%d-%b-%Y %I:%M:%S %p')
    df['value'] = df['Load']
    df = df[['datetime', 'value']].copy()
    df = df.sort_values('datetime').reset_index(drop=True)

    print(f"Real-time data: {len(df)} records from {df['datetime'].min()} to {df['datetime'].max()}")

    if len(df) > 0:
        full_range = pd.date_range(
            start=df['datetime'].min(),
            end=df['datetime'].max(),
            freq='15T'
        )
        full_df = pd.DataFrame({'datetime': full_range})
        merged_df = pd.merge(full_df, df, on='datetime', how='left')
        merged_df['value'] = merged_df['value'].interpolate(method='linear')
        merged_df['value'] = merged_df['value'].bfill().ffill()

        return merged_df[['datetime', 'value']]

    return df

# === Load Pre-trained Model and Scalers ===
print("Loading pre-trained model and scalers...")
model = TiDEModel.load("/content/final_tide_model.pkl")

with open("/content/final_scaler_y.pkl", "rb") as f:
    scaler_y = pickle.load(f)

with open("/content/final_scaler_future_cov.pkl", "rb") as f:
    scaler_future_cov = pickle.load(f)

# === Load Historical Data ===
print("Loading historical data...")
df_historical = pd.read_csv("/content/final_combined_timeseries.csv")
df_historical["datetime"] = pd.to_datetime(df_historical["datetime"])
df_historical.set_index("datetime", inplace=True)
df_historical = df_historical.sort_index()

print(f"Historical data range: {df_historical.index.min()} to {df_historical.index.max()}")

# === Merge with Latest Real-time Data ===
realtime_df = process_realtime_data("/content/brpl_data.txt")

if len(realtime_df) > 0:
    realtime_df.set_index('datetime', inplace=True)

    # Find where historical data ends
    historical_end = df_historical.index.max()

    # Get new real-time data after historical cutoff
    new_realtime = realtime_df.loc[realtime_df.index > historical_end]

    if len(new_realtime) > 0:
        print(f"Adding {len(new_realtime)} new real-time records from {new_realtime.index.min()} to {new_realtime.index.max()}")

        # Combine data
        df_combined = pd.concat([df_historical, new_realtime]).sort_index()
        df_combined = df_combined[~df_combined.index.duplicated(keep='last')]
    else:
        print("No new real-time data to add")
        df_combined = df_historical
else:
    print("No real-time data found, using historical data only")
    df_combined = df_historical

current_time = df_combined.index.max()
print(f"Current data extends to: {current_time}")
print(f"Combined data shape: {df_combined.shape}")

# === Calculate Output Chunk Length ===
target_time = pd.Timestamp("2025-07-02 07:00:00")  # Target 7 AM July 2nd
time_diff_hours = (target_time - current_time).total_seconds() / 3600
time_diff_steps = int(time_diff_hours * 4)  # 4 steps per hour (15-min intervals)

# Add 24 hours (96 steps) for the forecast period
total_steps_needed = time_diff_steps + 96
print(f"Time to target (7 AM): {time_diff_hours:.2f} hours ({time_diff_steps} steps)")
print(f"Total forecast steps needed: {total_steps_needed} (to reach 7 AM + 24 hours)")

# Ensure we don't exceed reasonable limits and it's at least the model's minimum
forecast_steps = max(164, total_steps_needed)  # At least 164 (model's output_chunk_length)
forecast_steps = min(forecast_steps, 300)  # Cap at reasonable limit

print(f"Using forecast horizon: {forecast_steps} steps ({forecast_steps * 15} minutes = {forecast_steps / 4:.1f} hours)")

# === Create Future Covariates (Historical + Future) ===
print("Creating future covariates...")

# Model needs covariates for input_chunk_length (124) + forecast steps
input_chunk_length = 124
covariate_start = current_time - pd.Timedelta(minutes=(input_chunk_length - 1) * 15)
covariate_end = current_time + pd.Timedelta(minutes=forecast_steps * 15)

print(f"Covariates needed from: {covariate_start} to {covariate_end}")

# Create complete timeline for covariates
covariate_timestamps = pd.date_range(
    start=covariate_start,
    end=covariate_end,
    freq='15T'
)

print(f"Total covariate timeline: {len(covariate_timestamps)} steps")

# Get weather data for the extended period
try:
    weather_data = Hourly(station_id, covariate_start, covariate_end + pd.Timedelta(hours=1)).fetch()
    weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres"]]

    # Interpolate to 15-minute frequency
    weather_15min = weather_data.reindex(covariate_timestamps)
    weather_15min = weather_15min.interpolate(method='linear', limit_direction='both')
    weather_15min = weather_15min.bfill().ffill()

    # Rename columns
    weather_15min.columns = [f"station_{station_id}_{col}" for col in weather_15min.columns]

    print(f"Weather data shape: {weather_15min.shape}")

except Exception as e:
    print(f"Weather fetch error: {e}, using default values")
    weather_15min = pd.DataFrame(index=covariate_timestamps)
    weather_15min[f"station_{station_id}_temp"] = 35.0
    weather_15min[f"station_{station_id}_dwpt"] = 25.0
    weather_15min[f"station_{station_id}_rhum"] = 60.0
    weather_15min[f"station_{station_id}_wspd"] = 5.0
    weather_15min[f"station_{station_id}_pres"] = 1013.0

# Create calendar features for the complete timeline
complete_calendar = create_calendar_features(covariate_timestamps)
complete_features = pd.concat([complete_calendar, weather_15min], axis=1)

# Add temperature squared
temp_col = f"station_{station_id}_temp"
if temp_col in complete_features.columns:
    complete_features[f"{temp_col}_squared"] = complete_features[temp_col] ** 2

print(f"Complete future covariates shape: {complete_features.shape}")

# === Generate Forecast ===
print("Generating forecast...")

# Historical series
historical_ts = TimeSeries.from_dataframe(df_combined, value_cols=["value"])
historical_scaled = scaler_y.transform(historical_ts).astype(np.float32)

# Complete future covariates (historical + future)
complete_cov_ts = TimeSeries.from_dataframe(complete_features)
complete_cov_scaled = scaler_future_cov.transform(complete_cov_ts).astype(np.float32)

# Generate forecast
forecast_scaled = model.predict(
    n=forecast_steps,
    series=historical_scaled,
    future_covariates=complete_cov_scaled
)

# Convert back to actual values
forecast = scaler_y.inverse_transform(forecast_scaled)
forecast_df = forecast.to_dataframe().reset_index()
forecast_df.columns = ['datetime', 'forecast_value']

print(f"Forecast generated: {len(forecast_df)} steps")
print(f"Forecast range: {forecast_df['datetime'].min()} to {forecast_df['datetime'].max()}")

# === Extract Last 96 Steps (24 hours) ===
forecast_24h = forecast_df.tail(96).copy().reset_index(drop=True)

print("=" * 60)
print("24-HOUR FORECAST RESULTS (LAST 96 STEPS)")
print("=" * 60)
print(f"Current time: {current_time}")
print(f"Target start (7 AM): {target_time}")
print(f"24-hour period: {forecast_24h['datetime'].min()} to {forecast_24h['datetime'].max()}")
print(f"Average Load: {forecast_24h['forecast_value'].mean():.1f} MW")
print(f"Peak Load: {forecast_24h['forecast_value'].max():.1f} MW")
print(f"Min Load: {forecast_24h['forecast_value'].min():.1f} MW")

# Save forecast
forecast_24h.to_csv("/content/forecast_24h_from_current.csv", index=False)
print(f"\nForecast saved to: /content/forecast_24h_from_current.csv")

# Display sample predictions
print(f"\nSample predictions:")
print(forecast_24h.head(8).to_string(index=False))
print("...")
print(forecast_24h.tail(8).to_string(index=False))

# Also save full forecast for reference
forecast_df.to_csv("/content/full_forecast.csv", index=False)
print(f"\nFull forecast saved to: /content/full_forecast.csv")

# === Load Best Model and Generate Validation Forecasts ===
import plotly.graph_objects as go
import numpy as np
from darts.metrics import mape, mae, rmse

# Load the best trained model
model = TiDEModel.load("/content/final_tide_model.pkl")

# Load scalers
with open("/content/final_scaler_y.pkl", "rb") as f:
    scaler_y = pickle.load(f)

with open("/content/final_scaler_future_cov.pkl", "rb") as f:
    scaler_future_cov = pickle.load(f)

print("Model and scalers loaded successfully!")

# === Define Start Date for Weekly Forecasts ===
start_date = pd.Timestamp("2024-01-01 07:00:00")  # Start of validation set at 7 AM

print(f"Validation data starts at: {final_val_ts.start_time()}")
print(f"Validation data ends at: {final_val_ts.end_time()}")
print(f"Requested start date: {start_date}")

# Verify the start date is within validation data and has enough input data
min_start_time = final_val_ts.start_time() + pd.Timedelta(hours=31)  # Need 31h of input data
if start_date < min_start_time:
    print(f"Warning: Start date is too early. Minimum start time is {min_start_time}")
    start_date = min_start_time

print(f"Using start date: {start_date}")

# === Generate One Week of Daily Forecasts ===
print("Generating 7 daily forecasts...")

# Calculate the end date for exactly 7 forecasts
end_date = start_date + pd.Timedelta(days=6)  # 6 days after start = 7 total days

# Check if we have enough data for the full week
latest_possible_start = final_val_ts.end_time() - pd.Timedelta(hours=40, minutes=45)
if end_date > latest_possible_start:
    end_date = latest_possible_start
    actual_days = int((end_date - start_date).total_seconds() / (24 * 3600)) + 1
    print(f"Adjusted to {actual_days} days due to data availability")
else:
    actual_days = 7

forecast_list = model.historical_forecasts(
    series=final_val_y,
    future_covariates=final_val_future_cov,
    start=start_date,
    forecast_horizon=164,  # 7 AM to 23:45 next day (40h 45m)
    stride=96,             # 24 hours = 96 steps at 15-minute intervals
    retrain=False,
    verbose=False,         # Reduce verbosity
    last_points_only=False
)

# Limit to exactly 7 forecasts (or fewer if less data available)
max_forecasts = min(7, len(forecast_list))
forecast_list = forecast_list[:max_forecasts]

print(f"Generated {len(forecast_list)} forecasts")

# === Process Each Forecast ===
weekly_forecasts = []
weekly_actuals = []
forecast_dates = []

for i, forecast in enumerate(forecast_list):
    print(f"Processing forecast {i+1}/{len(forecast_list)} from {forecast.start_time().strftime('%Y-%m-%d %H:%M')}")

    # Get actual values for the same period
    actual = final_val_y.slice(forecast.start_time(), forecast.end_time())

    # Convert to dataframes to check dimensions
    actual_df = actual_unscaled.to_dataframe()
    forecast_df = forecast_unscaled.to_dataframe()

    print(f"  Full forecast length: {len(forecast_df)} steps")
    print(f"  Full actual length: {len(actual_df)} steps")
    print(f"  Forecast time range: {forecast_df.index[0]} to {forecast_df.index[-1]}")

    # Extract last 24 hours for evaluation (last 96 steps)
    forecast_last24h = forecast_unscaled[-96:]
    actual_last24h = actual_unscaled[-96:]

    # Verify the extraction
    forecast_24h_df = forecast_last24h.to_dataframe()
    actual_24h_df = actual_last24h.to_dataframe()

    print(f"  Last 24h forecast: {forecast_24h_df.index[0]} to {forecast_24h_df.index[-1]}")
    print(f"  Last 24h actual: {actual_24h_df.index[0]} to {actual_24h_df.index[-1]}")

    # Store results
    weekly_forecasts.append(forecast_last24h)
    weekly_actuals.append(actual_last24h)
    forecast_dates.append(forecast.start_time())

    # Print daily metrics
    daily_mae = mae(actual_last24h, forecast_last24h)
    daily_mape = mape(actual_last24h, forecast_last24h)
    daily_rmse = rmse(actual_last24h, forecast_last24h)

    print(f"Day {i+1} - MAE: {daily_mae:.2f}, MAPE: {daily_mape:.2f}%, RMSE: {daily_rmse:.2f}")

# === Calculate Weekly Average Metrics ===
print(f"\n=== Weekly Summary (Last 24h of each forecast) ===")
weekly_mae = np.mean([mae(weekly_actuals[i], weekly_forecasts[i]) for i in range(len(weekly_forecasts))])
weekly_mape = np.mean([mape(weekly_actuals[i], weekly_forecasts[i]) for i in range(len(weekly_forecasts))])
weekly_rmse = np.mean([rmse(weekly_actuals[i], weekly_forecasts[i]) for i in range(len(weekly_forecasts))])

print(f"Average MAE: {weekly_mae:.2f}")
print(f"Average MAPE: {weekly_mape:.2f}%")
print(f"Average RMSE: {weekly_rmse:.2f}")

# === Plot All Weekly Forecasts on Same Plot ===
print("Creating visualization...")

# Create single figure
fig = go.Figure()

# Add all actual and forecast values with simple colors
for i, (forecast_24h, actual_24h) in enumerate(zip(weekly_forecasts, weekly_actuals)):
    # Convert to dataframes
    actual_df = actual_24h.pd_dataframe()
    forecast_df = forecast_24h.pd_dataframe()

    # Add actual values (show legend only for first trace)
    fig.add_trace(go.Scatter(
        x=actual_df.index,
        y=actual_df.iloc[:, 0],
        mode='lines',
        name='Actual' if i == 0 else None,
        line=dict(color='blue', width=2),
        legendgroup='actual',
        showlegend=(i == 0)
    ))

    # Add forecast values (show legend only for first trace)
    fig.add_trace(go.Scatter(
        x=forecast_df.index,
        y=forecast_df.iloc[:, 0],
        mode='lines',
        name='Forecast' if i == 0 else None,
        line=dict(color='red', width=2, dash='dash'),  # Added dash for forecasts
        legendgroup='forecast',
        showlegend=(i == 0)
    ))

# Update layout
fig.update_layout(
    title=f'TiDE Model: Weekly Validation Forecasts (Last 24h)<br>Starting from {start_date.strftime("%Y-%m-%d %H:%M")}<br>Avg MAE: {weekly_mae:.1f}, Avg MAPE: {weekly_mape:.1f}%, Avg RMSE: {weekly_rmse:.1f}',
    xaxis_title='Time',
    yaxis_title='Demand (MW)',
    hovermode='x unified',
    width=1400,
    height=700,
    template='plotly_white'
)

# Add grid for better readability
fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')
fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')

fig.show()

"""## Model - v2"""


# === Imports ===
import pandas as pd
import numpy as np
from datetime import datetime
from meteostat import Hourly
from darts import TimeSeries, concatenate
from darts.models import TiDEModel
from darts.metrics import mape, mae, rmse
from darts.dataprocessing.transformers import Scaler
from sklearn.preprocessing import MinMaxScaler
from pytorch_lightning.callbacks import EarlyStopping
import torch
import torch.nn as nn
import holidays
import warnings

warnings.filterwarnings("ignore")

torch.set_float32_matmul_precision('high')

# === Custom Loss Function ===
class CustomLast24hMAELoss(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, y_pred, y_true):
        # For 15-minute data, last 24h = 96 time steps (24 * 4)
        return torch.mean(torch.abs(y_true[:, -96:, :] - y_pred[:, -96:, :]))

# === Calendar Features ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add 15-minute specific features
    features["quarter_hour_sin"] = np.sin(2 * np.pi * index.minute / 60)
    features["quarter_hour_cos"] = np.cos(2 * np.pi * index.minute / 60)

    in_holidays = holidays.country_holidays("IN", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(in_holidays.keys()))).astype(int)
    return features

# === Load Electricity Demand (Keep at 15-minute frequency) ===
df = pd.read_csv("timeseries.csv")
df.columns = df.columns.str.strip()
df["ds"] = pd.to_datetime(df["ds"])
df.set_index("ds", inplace=True)
df = df.drop("unique_id", axis=1)
df = df.sort_index()

# Ensure 15-minute frequency
df_15min = df.copy()
print(f"Demand data shape at 15-minute frequency: {df_15min.shape}")

# === Load Weather Data with Linear Interpolation ===
# Weather data from Meteostat - not OpenMeteo
station_ids = ['42182']
start, end = df_15min.index.min(), df_15min.index.max()
weather_stations = {}

for station_id in station_ids:
    try:
        print(f"Fetching weather data for station: {station_id}")
        data = Hourly(station_id, start, end).fetch()
        data = data[["temp", "dwpt", "rhum", "wspd", "pres"]] #"prcp"

        # Fill missing values at hourly level using interpolation
        data = data.interpolate(method='linear', limit_direction='both')

        # Create 15-minute index aligned with demand data
        full_15min_index = df_15min.index

        # Reindex to 15-minute frequency and use linear interpolation
        data_15min = data.reindex(full_15min_index)
        data_15min = data_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values with backward fill (avoid forward looking)
        data_15min = data_15min.bfill()

        # Rename weather columns to include station ID
        data_15min.columns = [f"station_{station_id}_{col}" for col in data_15min.columns]

        weather_stations[station_id] = data_15min

        print(f"Station {station_id} - Weather features: {len(data_15min.columns)}")
        print(f"Missing values after interpolation: {data_15min.isnull().sum().sum()}")

    except Exception as e:
        print(f"Warning: Could not load data for station {station_id}. Error: {e}")

# Combine all weather stations into one DataFrame
weather_df = pd.concat(list(weather_stations.values()), axis=1)
print(f"Total weather features: {weather_df.shape[1]}")

# === Create Future Covariates (Calendar + Weather Features) ===
calendar_df = create_calendar_features(df_15min.index)

# Combine calendar and weather features as future covariates
future_features = pd.concat([calendar_df, weather_df], axis=1)

# Add weather interaction features
if len(station_ids) >= 2:
    station_1_temp = f"station_{station_ids[0]}_temp"
    station_2_temp = f"station_{station_ids[1]}_temp"

    if station_1_temp in future_features.columns and station_2_temp in future_features.columns:
        future_features["temp_diff"] = future_features[station_1_temp] - future_features[station_2_temp]
        future_features["temp_avg"] = (future_features[station_1_temp] + future_features[station_2_temp]) / 2

# Add squared terms for weather variables
weather_cols = [col for col in weather_df.columns]
for col in weather_cols:
    if 'temp' in col:
        future_features[f"{col}_squared"] = weather_df[col] ** 2

print(f"Total future covariate features: {future_features.shape[1]}")
print("Future covariates include:")
print(f"- Calendar features: {calendar_df.shape[1]}")
print(f"- Weather features: {weather_df.shape[1]}")
print(f"- Weather interactions: {future_features.shape[1] - calendar_df.shape[1] - weather_df.shape[1]}")

# === Create Darts TimeSeries ===
target_ts = TimeSeries.from_dataframe(df_15min, value_cols=["y"])
future_covariates_ts = TimeSeries.from_dataframe(future_features)

# Note: No past covariates since everything is treated as future covariates
print("No past covariates - all weather data treated as future covariates")

# === Train/Validation/Test Split ===
train_start = pd.Timestamp("2017-01-01 06:45:00")
train_end = pd.Timestamp("2024-01-01 07:00:00")
val_start = pd.Timestamp("2024-01-01 06:45:00")
val_end = pd.Timestamp("2025-01-01 07:00:00")
test_start = pd.Timestamp("2025-01-01 06:45:00")

# Target series splits
train_ts = target_ts.drop_before(train_start).drop_after(train_end)
val_ts = target_ts.drop_before(val_start).drop_after(val_end)
test_ts = target_ts.drop_before(test_start)

# Future covariates splits (calendar + weather features)
train_future_cov = future_covariates_ts.drop_before(train_start).drop_after(train_end)
val_future_cov = future_covariates_ts.drop_before(val_start).drop_after(val_end)
test_future_cov = future_covariates_ts.drop_before(test_start)

# Verification
print(f"Training data starts at: {train_ts.start_time()}")
print(f"Training data ends at: {train_ts.end_time()}")
print(f"Validation data starts at: {val_ts.start_time()}")
print(f"Validation data ends at: {val_ts.end_time()}")
print(f"Test data starts at: {test_ts.start_time()}")
print(f"Test data ends at: {test_ts.end_time()}")

# === Scale Target and Covariates ===
scaler_y = Scaler(scaler=MinMaxScaler())
scaler_future_cov = Scaler(scaler=MinMaxScaler())

# Scale target
train_y = scaler_y.fit_transform(train_ts).astype(np.float32)
val_y = scaler_y.transform(val_ts).astype(np.float32)
test_y = scaler_y.transform(test_ts).astype(np.float32)

# Scale future covariates (calendar + weather)
train_future_cov = scaler_future_cov.fit_transform(train_future_cov).astype(np.float32)
val_future_cov = scaler_future_cov.transform(val_future_cov).astype(np.float32)
test_future_cov = scaler_future_cov.transform(test_future_cov).astype(np.float32)

# === Configure TiDEModel ===
model = TiDEModel(
    input_chunk_length=192,      # 31 hours of input
    output_chunk_length=164,     # ~41 hours of output
    hidden_size=1024,
    temporal_decoder_hidden=32,
    temporal_width_past=4,
    temporal_width_future=4,
    num_encoder_layers=5,
    num_decoder_layers=5,
    decoder_output_dim=32,
    use_layer_norm=False,
    dropout=0.2,
    n_epochs=50,
    batch_size=64,
    optimizer_kwargs={"lr": 5e-4}, #1e-3
    loss_fn=CustomLast24hMAELoss(),
    pl_trainer_kwargs={
        "accelerator": "gpu",
        "devices": 1,
        "precision": "32-true",
        "gradient_clip_val": 0.5,
        "callbacks": [EarlyStopping(monitor="val_loss", patience=10)]
    },
    save_checkpoints=True,
    force_reset=True,
    random_state=42
)

# === Train the Model ===
print("Training model with all features as future covariates...")
print(f"Input chunk length: {model.input_chunk_length} steps ({model.input_chunk_length * 15} minutes)")
print(f"Output chunk length: {model.output_chunk_length} steps ({model.output_chunk_length * 15} minutes)")
print(f"Future covariates: {future_features.shape[1]} features")

model.fit(
    series=train_y,
    future_covariates=train_future_cov,
    val_series=val_y,
    val_future_covariates=val_future_cov,
    verbose=True
)

# === Load Best Model and Forecast ===
model = model.load_from_checkpoint(model.model_name, best=True)

forecast_list = model.historical_forecasts(
    series=test_y,
    future_covariates=test_future_cov,
    start=test_y.start_time(),
    forecast_horizon=164,
    stride=96,
    retrain=False,
    verbose=True,
    last_points_only=False
)

# === Extract and Evaluate Last 24 Hours ===
forecast_24 = [forecast[-96:] for forecast in forecast_list]
forecast_combined = concatenate(forecast_24)
actual_combined = test_y.slice(forecast_combined.start_time(), forecast_combined.end_time())
forecast_unscaled = scaler_y.inverse_transform(forecast_combined)
actual_unscaled = scaler_y.inverse_transform(actual_combined)

print("\n=== Evaluation Metrics ===")
print("MAE:", mae(actual_unscaled, forecast_unscaled))
print("MAPE:", mape(actual_unscaled, forecast_unscaled))
print("RMSE:", rmse(actual_unscaled, forecast_unscaled))

print("=" * 60)
print("FINAL MODEL TRAINING COMPLETED!")
print("=" * 60)

# === Save the Final Model and Scalers ===
print("Saving final model and scalers...")

# Save the trained model
model.save("/content/final_tide_model.pkl")

# Save scalers for future use
import pickle
with open("/content/final_scaler_y.pkl", "wb") as f:
    pickle.dump(scaler_y, f)

with open("/content/final_scaler_future_cov.pkl", "wb") as f:
    pickle.dump(scaler_future_cov, f)

print("Final model and scalers saved successfully!")
print("Files saved:")
print("- /content/final_tide_model.pkl")
print("- /content/final_scaler_y.pkl")
print("- /content/final_scaler_future_cov.pkl")

import pandas as pd
import numpy as np

def process_excel_file(file_path):
    """Process Excel file with multiple sheets"""
    print(f"Processing {file_path}...")
    all_data = []
    xls = pd.ExcelFile(file_path)

    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name)
        time_slots = df.iloc[:, 0]
        date_range = df.columns[1:]

        for i, date in enumerate(date_range):
            col_data = df.iloc[:, i + 1]
            full_datetimes = [pd.to_datetime(f"{date} {ts.split(' - ')[0]}") for ts in time_slots]

            temp_df = pd.DataFrame({
                'ds': full_datetimes,
                'y': col_data,
                'unique_id': 'delhi_demand'
            })
            all_data.append(temp_df)

    result = pd.concat(all_data, ignore_index=True)
    result = result.dropna().drop_duplicates(subset=['ds']).sort_values('ds')
    print(f"Extracted {len(result)} records")
    return result

def process_brpl_txt(file_path):
    """Process BRPL text file"""
    print(f"Processing {file_path}...")
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()
    df['Data_Time'] = df['Data_Time'].str.strip()
    df['ds'] = pd.to_datetime(df['Data_Time'], format='%d-%b-%Y %I:%M:%S %p')
    df['y'] = df['Load']
    df['unique_id'] = 'delhi_demand'
    result = df[['unique_id', 'ds', 'y']].dropna().sort_values('ds')
    print(f"Extracted {len(result)} records")
    return result

def interpolate_15min(df):
    """Interpolate to 15-minute frequency"""
    if len(df) == 0:
        return df

    # Create 15-minute range
    full_range = pd.date_range(start=df['ds'].min(), end=df['ds'].max(), freq='15T')
    full_df = pd.DataFrame({'ds': full_range, 'unique_id': 'delhi_demand'})

    # Merge and interpolate
    merged = pd.merge(full_df, df[['ds', 'y']], on='ds', how='left')
    merged['y'] = merged['y'].interpolate(method='linear').bfill().ffill()

    return merged[['unique_id', 'ds', 'y']]

# Main execution
print("Loading original timeseries...")
timeseries = pd.read_csv('/content/timeseries.csv')
timeseries['ds'] = pd.to_datetime(timeseries['ds'])

print("Processing April-May Excel...")
april_data = process_excel_file('/content/BRPL Demand 1st April-31st May (1).xlsx')

print("Processing June onwards Excel...")
june_data = process_excel_file('/content/BRPL Demand 1st June onwards.xlsx')

print("Processing BRPL text data...")
brpl_data = process_brpl_txt('/content/brpl_data.txt')

# Define cutoff points
timeseries_end = pd.Timestamp("2025-03-31 23:45:00")
excel_cutoff = pd.Timestamp("2025-07-01 10:00:00")

print("Combining data...")

# Start with original timeseries
combined = timeseries.copy()

# Add Excel data (April 1 to July 1 10:00 AM)
excel_combined = pd.concat([april_data, june_data]).drop_duplicates(subset=['ds']).sort_values('ds')
excel_filtered = excel_combined[(excel_combined['ds'] > timeseries_end) &
                               (excel_combined['ds'] <= excel_cutoff)]

if len(excel_filtered) > 0:
    excel_interpolated = interpolate_15min(excel_filtered)
    combined = pd.concat([combined, excel_interpolated])

# Add BRPL data (after July 1 10:00 AM)
brpl_filtered = brpl_data[brpl_data['ds'] > excel_cutoff]

if len(brpl_filtered) > 0:
    brpl_interpolated = interpolate_15min(brpl_filtered)
    combined = pd.concat([combined, brpl_interpolated])

# Final cleanup
combined = combined.drop_duplicates(subset=['ds']).sort_values('ds').reset_index(drop=True)
combined['y'] = combined['y'].interpolate(method='linear').bfill().ffill()

print(f"\nFinal result:")
print(f"Records: {len(combined)}")
print(f"Range: {combined['ds'].min()} to {combined['ds'].max()}")
print(f"Missing values: {combined['y'].isna().sum()}")

# Save result
combined.to_csv('/content/merged_timeseries.csv', index=False)
print("Saved to /content/merged_timeseries.csv")


import pandas as pd
from datetime import datetime, timedelta
from supabase import create_client, Client
import pytz

# === SUPABASE CONFIGURATION ===
supabase_url = 'https://ikuvbtsxjemnkfbgcjrl.supabase.co'
supabase_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImlrdXZidHN4amVtbmtmYmdjanJsIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTI4NzE5NjIsImV4cCI6MjA2ODQ0Nzk2Mn0.h7ofTtHrJx40A-SUP-f3v-6CbQd-eauFgx4Bhxg5Wtc'

# === CONNECT TO SUPABASE ===
print("Connecting to Supabase...")
supabase: Client = create_client(supabase_url, supabase_key)

# === FETCH LAST WEEK OF DATA ===
print("\n=== FETCHING LAST WEEK OF 15-MINUTE DATA FROM SUPABASE ===")

try:
    # First, get the latest timestamp to determine the end date
    print("Getting latest timestamp from database...")
    latest_response = supabase.table('actual_demand').select("timestamp").order('timestamp', desc=True).limit(1).execute()

    if latest_response.data:
        latest_timestamp = pd.to_datetime(latest_response.data[0]['timestamp'])
        print(f"Latest timestamp in database: {latest_timestamp}")

        # Calculate 1 week before the latest timestamp
        one_week_ago = latest_timestamp - timedelta(days=14)
        print(f"Fetching data from: {one_week_ago} to {latest_timestamp}")

        # Format timestamps for Supabase query
        start_str = one_week_ago.strftime('%Y-%m-%d %H:%M:%S')
        end_str = latest_timestamp.strftime('%Y-%m-%d %H:%M:%S')

        # Fetch data for the last week
        print("Fetching data...")
        # Supabase returns max 1000 rows by default, so we need to handle pagination
        all_data = []

        # First, get the count
        count_response = supabase.table('actual_demand').select("*", count='exact').gte('timestamp', start_str).lte('timestamp', end_str).execute()
        total_count = count_response.count
        print(f"Total records to fetch: {total_count}")

        # Fetch in batches if needed
        if total_count > 1000:
            batch_size = 1000
            for offset in range(0, total_count, batch_size):
                batch_response = (supabase.table('actual_demand')
                    .select("*")
                    .gte('timestamp', start_str)
                    .lte('timestamp', end_str)
                    .order('timestamp')
                    .range(offset, offset + batch_size - 1)
                    .execute())
                all_data.extend(batch_response.data)
                print(f"  Fetched batch: {offset} to {min(offset + batch_size, total_count)}")
        else:
            # If less than 1000 records, fetch normally
            response = (supabase.table('actual_demand')
                .select("*")
                .gte('timestamp', start_str)
                .lte('timestamp', end_str)
                .order('timestamp')
                .execute())
            all_data = response.data

        # Convert to DataFrame
        df = pd.DataFrame(all_data)

        # Format the data exactly like merged_timeseries.csv
        print("\nFormatting data...")
        formatted_df = pd.DataFrame()
        formatted_df['unique_id'] = 'delhi_demand'  # Constant value as per your original data
        formatted_df['ds'] = pd.to_datetime(df['timestamp'])
        formatted_df['y'] = df['actual_demand'].astype(float)

        # Sort by timestamp
        formatted_df = formatted_df.sort_values('ds').reset_index(drop=True)

        # Check for data consistency
        print(f"\n✓ Data fetched successfully!")
        print(f"  Records: {len(formatted_df)}")
        print(f"  Date range: {formatted_df['ds'].min()} to {formatted_df['ds'].max()}")
        print(f"  Missing values in 'y': {formatted_df['y'].isna().sum()}")

        # Check frequency
        if len(formatted_df) > 1:
            time_diffs = formatted_df['ds'].diff().dropna()
            most_common_diff = time_diffs.mode()[0]
            print(f"  Most common time interval: {most_common_diff}")

            # Check if it's 15-minute frequency
            expected_15min = pd.Timedelta(minutes=15)
            if most_common_diff == expected_15min:
                print("  ✓ Data is at 15-minute frequency")
            else:
                print(f"  ⚠ Warning: Data may not be at exact 15-minute frequency")

                # Optional: Resample to ensure 15-minute frequency
                print("\n  Resampling to ensure 15-minute frequency...")
                formatted_df['ds'] = pd.to_datetime(formatted_df['ds'])
                formatted_df = formatted_df.set_index('ds')

                # Create complete 15-minute index
                full_index = pd.date_range(
                    start=formatted_df.index.min(),
                    end=formatted_df.index.max(),
                    freq='15T'
                )

                # Reindex and interpolate
                formatted_df = formatted_df.reindex(full_index)
                formatted_df['y'] = formatted_df['y'].interpolate(method='linear').bfill().ffill()
                formatted_df['unique_id'] = 'delhi_demand'

                # Reset index to match format
                formatted_df = formatted_df.reset_index()
                formatted_df.columns = ['ds', 'y', 'unique_id']
                formatted_df = formatted_df[['unique_id', 'ds', 'y']]  # Reorder columns

                print(f"  ✓ Resampled to {len(formatted_df)} records at 15-minute intervals")

        # Save to CSV
        output_file = 'last_week_timeseries.csv'
        formatted_df.to_csv(output_file, index=False)
        print(f"\n✓ Data saved to '{output_file}'")

        # Display sample of the data
        print("\nFirst 10 rows:")
        print(formatted_df.head(10))
        print("\nLast 10 rows:")
        print(formatted_df.tail(10))

        # Summary statistics
        print("\nSummary statistics for 'y' (demand):")
        print(formatted_df['y'].describe())

    else:
        print("No data found in the actual_demand table!")

except Exception as e:
    print(f"Error fetching data from Supabase: {e}")
    raise

print("\n=== PROCESS COMPLETED ===")

# === DEPLOYMENT SCRIPT FOR TIDE MODEL FORECASTING ===
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from meteostat import Hourly
from darts import TimeSeries
from darts.models import TiDEModel
import torch
import holidays
import pickle
import warnings

warnings.filterwarnings("ignore")

print("=" * 60)
print("STARTING DEPLOYMENT FOR ELECTRICITY DEMAND FORECASTING")
print("=" * 60)

# === Calendar Features Function ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add 15-minute specific features
    features["quarter_hour_sin"] = np.sin(2 * np.pi * index.minute / 60)
    features["quarter_hour_cos"] = np.cos(2 * np.pi * index.minute / 60)

    in_holidays = holidays.country_holidays("IN", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(in_holidays.keys()))).astype(int)
    return features

print("\n=== LOADING TRAINED MODEL AND SCALERS ===")

# Load the trained model
print("Loading trained model...")
model = TiDEModel.load("/content/final_tide_model.pkl")
print("✓ Model loaded successfully")

# Load scalers
print("Loading scalers...")
with open("/content/final_scaler_y.pkl", "rb") as f:
    scaler_y = pickle.load(f)

with open("/content/final_scaler_future_cov.pkl", "rb") as f:
    scaler_future_cov = pickle.load(f)

print("✓ Scalers loaded successfully")

print("\n=== LOADING UPDATED DEMAND DATA ===")

# Load updated demand data
df = pd.read_csv("last_week_timeseries.csv")
df.columns = df.columns.str.strip()
df["ds"] = pd.to_datetime(df["ds"])
df.set_index("ds", inplace=True)
# keep timestamps till 2025-08-19 06:45:00
df = df[df.index <= pd.to_datetime("2025-08-19 06:45:00")]

# Drop unique_id if it exists
if "unique_id" in df.columns:
    df = df.drop("unique_id", axis=1)

# Create a complete 15-minute frequency index for the entire range
complete_index = pd.date_range(
    start=df.index.min(),
    end=df.index.max(),
    freq='15T'
)

# Reindex to ensure all 15-minute intervals are present
df = df.reindex(complete_index)

# Fill missing values using interpolation
df['y'] = df['y'].interpolate(method='linear')

# Handle any remaining NaN values at the beginning or end
df['y'] = df['y'].bfill().ffill()

# Sort index to ensure proper ordering
df = df.sort_index()

print(f"✓ Data after interpolation:")
print(f"  Shape: {df.shape}")
print(f"  Range: {df.index.min()} to {df.index.max()}")
print(f"  Total timesteps: {len(df)}")
print(f"  Missing values: {df['y'].isna().sum()}")

# Drop unique_id if it exists
if "unique_id" in df.columns:
    df = df.drop("unique_id", axis=1)

df = df.sort_index()

print(f"✓ Updated demand data loaded")
print(f"  Shape: {df.shape}")
print(f"  Range: {df.index.min()} to {df.index.max()}")

# Get the last timestamp
last_timestamp = df.index.max()
forecast_start = last_timestamp
print(f"  Last timestamp (forecast start): {forecast_start}")

print("\n=== SETTING UP FORECAST PARAMETERS ===")

# Model parameters
input_length = 192  # timesteps (31 hours)
forecast_length = 164  # timesteps (41 hours)

# Calculate required data range
# We need 124 timesteps BEFORE the forecast start
input_start = forecast_start - pd.Timedelta(minutes=(input_length - 1) * 15)
forecast_end = forecast_start + pd.Timedelta(minutes=forecast_length * 15)

print(f"✓ Forecast parameters:")
print(f"  Input data needed from: {input_start}")
print(f"  Forecast start: {forecast_start}")
print(f"  Forecast end: {forecast_end}")
print(f"  Input length: {input_length} timesteps ({input_length * 15 / 60:.1f} hours)")
print(f"  Forecast length: {forecast_length} timesteps ({forecast_length * 15 / 60:.1f} hours)")

print("\n=== CREATING EXTENDED TIME INDEX ===")

# Create extended index for features (input period + forecast period + buffer)
feature_end = forecast_end + pd.Timedelta(hours=6)  # Extra buffer
extended_index = pd.date_range(
    start=input_start,
    end=feature_end,
    freq='15min'
)

print(f"✓ Extended index created:")
print(f"  Range: {extended_index.min()} to {extended_index.max()}")
print(f"  Length: {len(extended_index)} timesteps")

print("\n=== GENERATING CALENDAR FEATURES ===")

calendar_features = create_calendar_features(extended_index)
print(f"✓ Calendar features generated: {calendar_features.shape}")

print("\n=== FETCHING WEATHER DATA ===")

# Use the same station ID as in training
station_ids = ['42182']
weather_stations = {}

for station_id in station_ids:
    try:
        print(f"  Fetching weather data for station: {station_id}")

        # Fetch weather data with buffer
        weather_start = input_start - pd.Timedelta(hours=12)
        weather_end = feature_end

        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres"]]

        print(f"    Raw weather data shape: {weather_data.shape}")

        # Fill missing values at hourly level
        weather_data = weather_data.interpolate(method='linear', limit_direction='both')

        # Reindex to 15-minute frequency and interpolate
        weather_15min = weather_data.reindex(extended_index)
        weather_15min = weather_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values
        weather_15min = weather_15min.bfill().ffill()

        # Rename columns to match training format
        weather_15min.columns = [f"station_{station_id}_{col}" for col in weather_15min.columns]

        weather_stations[station_id] = weather_15min

        print(f"    ✓ Station {station_id}: {weather_15min.shape[1]} features, {weather_15min.isnull().sum().sum()} missing values")

    except Exception as e:
        print(f"    ✗ Error loading station {station_id}: {e}")

# Combine weather data
weather_df = pd.concat(list(weather_stations.values()), axis=1)
print(f"✓ Total weather features: {weather_df.shape[1]}")

print("\n=== CREATING FUTURE COVARIATES ===")

# Combine calendar and weather features
future_features = pd.concat([calendar_features, weather_df], axis=1)

# Add the same feature engineering as in training
# Add squared terms for temperature variables
weather_cols = [col for col in weather_df.columns if 'temp' in col]
for col in weather_cols:
    future_features[f"{col}_squared"] = weather_df[col] ** 2

print(f"✓ Future covariates created:")
print(f"  Total features: {future_features.shape[1]}")
print(f"  Calendar features: {calendar_features.shape[1]}")
print(f"  Weather features: {weather_df.shape[1]}")
print(f"  Engineered features: {future_features.shape[1] - calendar_features.shape[1] - weather_df.shape[1]}")

print("\n=== PREPARING DATA FOR PREDICTION ===")

# Create the input target series (historical data up to forecast start)
input_data = df.loc[input_start:forecast_start].copy()

# Check if we have enough input data
if len(input_data) < input_length:
    print(f"✗ Error: Need {input_length} timesteps, but only have {len(input_data)}")
    print(f"  Available data range: {input_data.index.min()} to {input_data.index.max()}")
    raise ValueError("Insufficient historical data for prediction")

print(f"✓ Input data prepared:")
print(f"  Shape: {input_data.shape}")
print(f"  Range: {input_data.index.min()} to {input_data.index.max()}")

# Create TimeSeries objects
target_ts = TimeSeries.from_dataframe(input_data, value_cols=["y"])
future_covariates_ts = TimeSeries.from_dataframe(future_features)

print(f"✓ TimeSeries objects created:")
print(f"  Target: {target_ts.start_time()} to {target_ts.end_time()} ({len(target_ts)} timesteps)")
print(f"  Future covariates: {future_covariates_ts.start_time()} to {future_covariates_ts.end_time()} ({len(future_covariates_ts)} timesteps)")

print("\n=== SCALING DATA ===")

# Scale the input data using the fitted scalers
scaled_target = scaler_y.transform(target_ts).astype(np.float32)
scaled_future_cov = scaler_future_cov.transform(future_covariates_ts).astype(np.float32)

print("✓ Data scaled successfully")

print("\n=== MAKING FORECAST ===")

try:
    print(f"Making {forecast_length}-step forecast...")

    # Make prediction
    forecast_scaled = model.predict(
        n=forecast_length,
        series=scaled_target,
        future_covariates=scaled_future_cov
    )

    # Inverse transform the forecast
    forecast = scaler_y.inverse_transform(forecast_scaled)

    print("✓ Forecast completed successfully!")

    # Convert to DataFrame for easier handling
    forecast_df = forecast.to_dataframe()
    forecast_df.columns = ['demand_forecast']

    print(f"✓ Forecast range: {forecast_df.index.min()} to {forecast_df.index.max()}")
    print(f"✓ Forecast length: {len(forecast_df)} timesteps")

except Exception as e:
    print(f"✗ Error during forecasting: {e}")
    raise

print("\n=== SAVING RESULTS ===")

# Save forecast results
forecast_df.to_csv("electricity_demand_forecast.csv")
print("✓ Forecast saved to 'electricity_demand_forecast.csv'")

# Create summary statistics
summary_stats = {
    'forecast_start': forecast_start,
    'forecast_end': forecast_df.index.max(),
    'num_timesteps': len(forecast_df),
    'min_demand': forecast_df['demand_forecast'].min(),
    'max_demand': forecast_df['demand_forecast'].max(),
    'mean_demand': forecast_df['demand_forecast'].mean(),
    'std_demand': forecast_df['demand_forecast'].std()
}

print("\n=== FORECAST SUMMARY ===")
for key, value in summary_stats.items():
    if isinstance(value, float):
        print(f"{key}: {value:.2f}")
    else:
        print(f"{key}: {value}")

print("\n=== SAMPLE FORECAST VALUES ===")
print("First 10 predictions:")
print(forecast_df.head(10))
print("\nLast 10 predictions:")
print(forecast_df.tail(10))

print("\n" + "=" * 60)
print("DEPLOYMENT COMPLETED SUCCESSFULLY!")
print("=" * 60)
print(f"Forecast file: electricity_demand_forecast.csv")
print(f"Forecast period: {forecast_start} to {forecast_df.index.max()}")
print(f"Total timesteps: {len(forecast_df)} (≈{len(forecast_df) * 15 / 60:.1f} hours)")

# Drop unique_id if it exists
if "unique_id" in df.columns:
    df = df.drop("unique_id", axis=1)

df = df.sort_index()

print(f"✓ Updated demand data loaded")
print(f"  Shape: {df.shape}")
print(f"  Range: {df.index.min()} to {df.index.max()}")

# Get the last timestamp
last_timestamp = df.index.max()
forecast_start = last_timestamp
print(f"  Last timestamp (forecast start): {forecast_start}")

print("\n=== SETTING UP FORECAST PARAMETERS ===")

# Model parameters
input_length = 192  # timesteps (31 hours)
forecast_length = 164  # timesteps (41 hours)

# Calculate required data range
# We need 124 timesteps BEFORE the forecast start
input_start = forecast_start - pd.Timedelta(minutes=(input_length - 1) * 15)
forecast_end = forecast_start + pd.Timedelta(minutes=forecast_length * 15)

print(f"✓ Forecast parameters:")
print(f"  Input data needed from: {input_start}")
print(f"  Forecast start: {forecast_start}")
print(f"  Forecast end: {forecast_end}")
print(f"  Input length: {input_length} timesteps ({input_length * 15 / 60:.1f} hours)")
print(f"  Forecast length: {forecast_length} timesteps ({forecast_length * 15 / 60:.1f} hours)")

print("\n=== CREATING EXTENDED TIME INDEX ===")

# Create extended index for features (input period + forecast period + buffer)
feature_end = forecast_end + pd.Timedelta(hours=6)  # Extra buffer
extended_index = pd.date_range(
    start=input_start,
    end=feature_end,
    freq='15min'
)

print(f"✓ Extended index created:")
print(f"  Range: {extended_index.min()} to {extended_index.max()}")
print(f"  Length: {len(extended_index)} timesteps")

print("\n=== GENERATING CALENDAR FEATURES ===")

calendar_features = create_calendar_features(extended_index)
print(f"✓ Calendar features generated: {calendar_features.shape}")

print("\n=== FETCHING WEATHER DATA ===")

# Use the same station ID as in training
station_ids = ['42182']
weather_stations = {}

for station_id in station_ids:
    try:
        print(f"  Fetching weather data for station: {station_id}")

        # Fetch weather data with buffer
        weather_start = input_start - pd.Timedelta(hours=12)
        weather_end = feature_end

        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres"]]

        print(f"    Raw weather data shape: {weather_data.shape}")

        # Fill missing values at hourly level
        weather_data = weather_data.interpolate(method='linear', limit_direction='both')

        # Reindex to 15-minute frequency and interpolate
        weather_15min = weather_data.reindex(extended_index)
        weather_15min = weather_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values
        weather_15min = weather_15min.bfill().ffill()

        # Rename columns to match training format
        weather_15min.columns = [f"station_{station_id}_{col}" for col in weather_15min.columns]

        weather_stations[station_id] = weather_15min

        print(f"    ✓ Station {station_id}: {weather_15min.shape[1]} features, {weather_15min.isnull().sum().sum()} missing values")

    except Exception as e:
        print(f"    ✗ Error loading station {station_id}: {e}")

# Combine weather data
weather_df = pd.concat(list(weather_stations.values()), axis=1)
print(f"✓ Total weather features: {weather_df.shape[1]}")

print("\n=== CREATING FUTURE COVARIATES ===")

# Combine calendar and weather features
future_features = pd.concat([calendar_features, weather_df], axis=1)

# Add the same feature engineering as in training
# Add squared terms for temperature variables
weather_cols = [col for col in weather_df.columns if 'temp' in col]
for col in weather_cols:
    future_features[f"{col}_squared"] = weather_df[col] ** 2

print(f"✓ Future covariates created:")
print(f"  Total features: {future_features.shape[1]}")
print(f"  Calendar features: {calendar_features.shape[1]}")
print(f"  Weather features: {weather_df.shape[1]}")
print(f"  Engineered features: {future_features.shape[1] - calendar_features.shape[1] - weather_df.shape[1]}")

print("\n=== PREPARING DATA FOR PREDICTION ===")

# Create the input target series (historical data up to forecast start)
input_data = df.loc[input_start:forecast_start].copy()

# Check if we have enough input data
if len(input_data) < input_length:
    print(f"✗ Error: Need {input_length} timesteps, but only have {len(input_data)}")
    print(f"  Available data range: {input_data.index.min()} to {input_data.index.max()}")
    raise ValueError("Insufficient historical data for prediction")

print(f"✓ Input data prepared:")
print(f"  Shape: {input_data.shape}")
print(f"  Range: {input_data.index.min()} to {input_data.index.max()}")

# Create TimeSeries objects
target_ts = TimeSeries.from_dataframe(input_data, value_cols=["y"])
future_covariates_ts = TimeSeries.from_dataframe(future_features)

print(f"✓ TimeSeries objects created:")
print(f"  Target: {target_ts.start_time()} to {target_ts.end_time()} ({len(target_ts)} timesteps)")
print(f"  Future covariates: {future_covariates_ts.start_time()} to {future_covariates_ts.end_time()} ({len(future_covariates_ts)} timesteps)")

print("\n=== SCALING DATA ===")

# Scale the input data using the fitted scalers
scaled_target = scaler_y.transform(target_ts).astype(np.float32)
scaled_future_cov = scaler_future_cov.transform(future_covariates_ts).astype(np.float32)

print("✓ Data scaled successfully")

print("\n=== MAKING FORECAST ===")

try:
    print(f"Making {forecast_length}-step forecast...")

    # Make prediction
    forecast_scaled = model.predict(
        n=forecast_length,
        series=scaled_target,
        future_covariates=scaled_future_cov
    )

    # Inverse transform the forecast
    forecast = scaler_y.inverse_transform(forecast_scaled)

    print("✓ Forecast completed successfully!")

    # Convert to DataFrame for easier handling
    forecast_df = forecast.to_dataframe()
    forecast_df.columns = ['demand_forecast']

    print(f"✓ Forecast range: {forecast_df.index.min()} to {forecast_df.index.max()}")
    print(f"✓ Forecast length: {len(forecast_df)} timesteps")

except Exception as e:
    print(f"✗ Error during forecasting: {e}")
    raise

print("\n=== SAVING RESULTS ===")

# Save forecast results
forecast_df.to_csv("electricity_demand_forecast.csv")
print("✓ Forecast saved to 'electricity_demand_forecast.csv'")

# Create summary statistics
summary_stats = {
    'forecast_start': forecast_start,
    'forecast_end': forecast_df.index.max(),
    'num_timesteps': len(forecast_df),
    'min_demand': forecast_df['demand_forecast'].min(),
    'max_demand': forecast_df['demand_forecast'].max(),
    'mean_demand': forecast_df['demand_forecast'].mean(),
    'std_demand': forecast_df['demand_forecast'].std()
}

print("\n=== FORECAST SUMMARY ===")
for key, value in summary_stats.items():
    if isinstance(value, float):
        print(f"{key}: {value:.2f}")
    else:
        print(f"{key}: {value}")

print("\n=== SAMPLE FORECAST VALUES ===")
print("First 10 predictions:")
print(forecast_df.head(10))
print("\nLast 10 predictions:")
print(forecast_df.tail(10))

print("\n" + "=" * 60)
print("DEPLOYMENT COMPLETED SUCCESSFULLY!")
print("=" * 60)
print(f"Forecast file: electricity_demand_forecast.csv")
print(f"Forecast period: {forecast_start} to {forecast_df.index.max()}")
print(f"Total timesteps: {len(forecast_df)} (≈{len(forecast_df) * 15 / 60:.1f} hours)")

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import openpyxl
from openpyxl.styles import Font, Alignment, Border, Side, PatternFill

def create_forecast_excel(forecast_csv_path="electricity_demand_forecast.csv",
                         output_path="Daily_Electricity_Demand_Forecast.xlsx"):
    """
    Convert the electricity demand forecast to Excel format with 15-minute time slots
    for the following day only.
    """

    print("Loading forecast data...")
    # Load the forecast data
    forecast_df = pd.read_csv(forecast_csv_path, index_col=0, parse_dates=True)

    # Get the first full day from the forecast
    first_timestamp = forecast_df.index[0]
    next_day = first_timestamp.date() + timedelta(days=1)

    # Filter for the following day only (24 hours = 96 fifteen-minute intervals)
    start_of_day = pd.Timestamp(next_day)
    end_of_day = start_of_day + timedelta(days=1) - timedelta(minutes=15)

    # Filter the forecast for the specific day
    daily_forecast = forecast_df[(forecast_df.index >= start_of_day) &
                                (forecast_df.index <= end_of_day)]

    print(f"Creating forecast for date: {next_day}")
    print(f"Number of time slots: {len(daily_forecast)}")

    # Create time slot labels
    time_slots = []
    for hour in range(24):
        for minute in [0, 15, 30, 45]:
            start_time = f"{hour:02d}:{minute:02d}"
            end_minute = minute + 15
            if end_minute == 60:
                end_hour = hour + 1
                end_minute = 0
            else:
                end_hour = hour

            if end_hour == 24:
                end_time = "24:00"
            else:
                end_time = f"{end_hour:02d}:{end_minute:02d}"

            time_slots.append(f"{start_time} - {end_time}")

    # Create the Excel data
    excel_data = {
        'Time Slot': time_slots,
        'Demand Forecast (MW)': [None] * 96  # Initialize with None
    }

    # Fill in the actual forecast values
    for i, (timestamp, row) in enumerate(daily_forecast.iterrows()):
        if i < 96:  # Ensure we don't exceed 96 time slots
            excel_data['Demand Forecast (MW)'][i] = round(row.iloc[0], 2)

    # Create DataFrame
    result_df = pd.DataFrame(excel_data)

    # Create Excel file with formatting
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        result_df.to_excel(writer, sheet_name='Daily Forecast', index=False)

        # Get the workbook and worksheet
        workbook = writer.book
        worksheet = writer.sheets['Daily Forecast']

        # Define styles
        header_font = Font(bold=True, size=12, color="FFFFFF")
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        data_font = Font(size=10)
        border = Border(
            left=Side(border_style="thin"),
            right=Side(border_style="thin"),
            top=Side(border_style="thin"),
            bottom=Side(border_style="thin")
        )
        center_alignment = Alignment(horizontal="center", vertical="center")

        # Apply header formatting
        for col in range(1, 3):  # Columns A and B
            cell = worksheet.cell(row=1, column=col)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = center_alignment
            cell.border = border

        # Apply data formatting
        for row in range(2, 98):  # Rows 2 to 97 (96 time slots + header)
            for col in range(1, 3):
                cell = worksheet.cell(row=row, column=col)
                cell.font = data_font
                cell.border = border
                if col == 1:  # Time slot column
                    cell.alignment = center_alignment
                else:  # Demand forecast column
                    cell.alignment = Alignment(horizontal="right", vertical="center")

        # Adjust column widths
        worksheet.column_dimensions['A'].width = 18
        worksheet.column_dimensions['B'].width = 20

        # Add title and metadata
        worksheet.insert_rows(1)
        title_cell = worksheet.cell(row=1, column=1)
        title_cell.value = f"Electricity Demand Forecast - {next_day.strftime('%Y-%m-%d')}"
        title_cell.font = Font(bold=True, size=14)
        worksheet.merge_cells('A1:B1')

        # Add generation timestamp
        worksheet.insert_rows(2)
        timestamp_cell = worksheet.cell(row=2, column=1)
        timestamp_cell.value = f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        timestamp_cell.font = Font(size=10, italic=True)
        worksheet.merge_cells('A2:B2')

        # Add empty row for spacing
        worksheet.insert_rows(3)

    print(f"✓ Excel file created: {output_path}")
    print(f"✓ Forecast date: {next_day}")
    print(f"✓ Time slots: 96 (15-minute intervals)")

    # Display summary statistics
    forecast_values = [x for x in excel_data['Demand Forecast (MW)'] if x is not None]
    if forecast_values:
        print(f"\nForecast Summary:")
        print(f"  Minimum demand: {min(forecast_values):.2f} MW")
        print(f"  Maximum demand: {max(forecast_values):.2f} MW")
        print(f"  Average demand: {np.mean(forecast_values):.2f} MW")
        print(f"  Peak time slot: {time_slots[forecast_values.index(max(forecast_values))]}")
        print(f"  Low time slot: {time_slots[forecast_values.index(min(forecast_values))]}")

    return output_path

def display_sample_data(forecast_csv_path="electricity_demand_forecast.csv"):
    """
    Display a sample of the forecast data to verify the conversion
    """
    try:
        forecast_df = pd.read_csv(forecast_csv_path, index_col=0, parse_dates=True)
        first_timestamp = forecast_df.index[0]
        next_day = first_timestamp.date() + timedelta(days=1)

        start_of_day = pd.Timestamp(next_day)
        end_of_day = start_of_day + timedelta(days=1) - timedelta(minutes=15)

        daily_forecast = forecast_df[(forecast_df.index >= start_of_day) &
                                    (forecast_df.index <= end_of_day)]

        print(f"\nSample forecast data for {next_day}:")
        print("=" * 50)
        print(daily_forecast.head(12))  # Show first 3 hours (12 intervals)
        print("...")
        print(daily_forecast.tail(12))   # Show last 3 hours (12 intervals)

        return True
    except Exception as e:
        print(f"Could not load forecast data: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("ELECTRICITY DEMAND FORECAST - EXCEL GENERATOR")
    print("=" * 60)

    # First, try to display sample data
    if display_sample_data():
        # Create the Excel file
        output_file = create_forecast_excel()

        print("\n" + "=" * 60)
        print("EXCEL GENERATION COMPLETED!")
        print("=" * 60)
        print(f"Output file: {output_file}")
        print("\nThe Excel file contains:")
        print("• Time slots in 15-minute intervals (00:00-00:15 to 23:45-24:00)")
        print("• Electricity demand forecasts for the following day")
        print("• Professional formatting with headers and borders")
        print("• Summary statistics in the console output")
    else:
        print("\nPlease ensure 'electricity_demand_forecast.csv' exists in the current directory.")
        print("Run the deployment script first to generate the forecast data.")

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import openpyxl
from openpyxl.styles import Font, Alignment, Border, Side, PatternFill
from meteostat import Hourly

def create_weather_excel(forecast_csv_path="electricity_demand_forecast.csv",
                        output_path="Daily_Weather_Forecast.xlsx",
                        station_id='42182'):
    """
    Create an Excel file with weather data for the same 24-hour period as the electricity forecast.
    Uses 15-minute intervals to match the electricity demand forecast.
    Note: Precipitation is NOT interpolated to preserve zero values.
    """

    print("Loading forecast data to determine date range...")

    # Load the forecast data to get the date range
    try:
        forecast_df = pd.read_csv(forecast_csv_path, index_col=0, parse_dates=True)
        first_timestamp = forecast_df.index[0]
        next_day = first_timestamp.date() + timedelta(days=1)

        print(f"Target forecast date: {next_day}")
    except Exception as e:
        print(f"Error loading forecast data: {e}")
        print("Using current date + 1 as fallback...")
        next_day = datetime.now().date() + timedelta(days=1)

    # Define the 24-hour period (96 fifteen-minute intervals)
    start_of_day = pd.Timestamp(next_day)
    end_of_day = start_of_day + timedelta(days=1) - timedelta(minutes=15)

    print(f"Fetching weather data for: {start_of_day} to {end_of_day}")

    # Create 15-minute time index
    time_index_15min = pd.date_range(
        start=start_of_day,
        end=end_of_day,
        freq='15min'
    )

    # Fetch hourly weather data with some buffer
    weather_start = start_of_day - timedelta(hours=2)
    weather_end = end_of_day + timedelta(hours=2)

    try:
        print(f"Fetching weather data from station {station_id}...")
        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres", "prcp"]]

        print(f"Raw weather data shape: {weather_data.shape}")

        # Separate precipitation from other variables
        prcp_data = weather_data[['prcp']].copy()
        other_weather = weather_data.drop('prcp', axis=1).copy()

        # Fill missing values for non-precipitation variables
        other_weather = other_weather.interpolate(method='linear', limit_direction='both')

        # For precipitation, use forward fill only (don't interpolate between zeros)
        prcp_data = prcp_data.fillna(method='ffill').fillna(0.0)

        # Reindex to 15-minute frequency
        other_weather_15min = other_weather.reindex(time_index_15min)
        prcp_15min = prcp_data.reindex(time_index_15min)

        # Interpolate non-precipitation variables to 15-minute intervals
        other_weather_15min = other_weather_15min.interpolate(method='linear', limit_direction='both')
        other_weather_15min = other_weather_15min.bfill().ffill()

        # For precipitation, use forward fill to 15-minute intervals (no interpolation)
        prcp_15min = prcp_15min.fillna(method='ffill').fillna(0.0)

        # Combine back together
        weather_15min = pd.concat([other_weather_15min, prcp_15min], axis=1)

        print(f"15-minute weather data shape: {weather_15min.shape}")
        print(f"Missing values: {weather_15min.isnull().sum().sum()}")
        print(f"Precipitation zeros preserved: {(weather_15min['prcp'] == 0).sum()} out of {len(weather_15min)} intervals")

    except Exception as e:
        print(f"✗ Error fetching weather data: {e}")
        print("Unable to proceed without valid weather data from the API.")
        raise Exception("Weather data fetch failed. Please check your internet connection and try again.")

    # Create time slot labels (same as electricity forecast)
    time_slots = []
    for hour in range(24):
        for minute in [0, 15, 30, 45]:
            start_time = f"{hour:02d}:{minute:02d}"
            end_minute = minute + 15
            if end_minute == 60:
                end_hour = hour + 1
                end_minute = 0
            else:
                end_hour = hour

            if end_hour == 24:
                end_time = "24:00"
            else:
                end_time = f"{end_hour:02d}:{end_minute:02d}"

            time_slots.append(f"{start_time} - {end_time}")

    # Prepare Excel data
    excel_data = {
        'Time Slot': time_slots,
        'Temperature (°C)': [round(val, 1) for val in weather_15min['temp'].values],
        'Dew Point (°C)': [round(val, 1) for val in weather_15min['dwpt'].values],
        'Humidity (%)': [round(val, 1) for val in weather_15min['rhum'].values],
        'Wind Speed (km/h)': [round(val, 1) for val in weather_15min['wspd'].values],
        'Pressure (hPa)': [round(val, 1) for val in weather_15min['pres'].values],
        'Precipitation (mm)': [round(val, 1) for val in weather_15min['prcp'].values]
    }

    # Create DataFrame
    result_df = pd.DataFrame(excel_data)

    # Create Excel file with formatting
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        result_df.to_excel(writer, sheet_name='Daily Weather', index=False)

        # Get the workbook and worksheet
        workbook = writer.book
        worksheet = writer.sheets['Daily Weather']

        # Define styles
        header_font = Font(bold=True, size=12, color="FFFFFF")
        header_fill = PatternFill(start_color="2E8B57", end_color="2E8B57", fill_type="solid")  # Sea green
        data_font = Font(size=10)
        border = Border(
            left=Side(border_style="thin"),
            right=Side(border_style="thin"),
            top=Side(border_style="thin"),
            bottom=Side(border_style="thin")
        )
        center_alignment = Alignment(horizontal="center", vertical="center")

        # Apply header formatting
        for col in range(1, 8):  # Columns A through G
            cell = worksheet.cell(row=1, column=col)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = center_alignment
            cell.border = border

        # Apply data formatting
        for row in range(2, 98):  # Rows 2 to 97 (96 time slots + header)
            for col in range(1, 8):  # Columns A through G
                cell = worksheet.cell(row=row, column=col)
                cell.font = data_font
                cell.border = border
                if col == 1:  # Time slot column
                    cell.alignment = center_alignment
                else:  # Weather data columns
                    cell.alignment = Alignment(horizontal="right", vertical="center")

        # Adjust column widths
        worksheet.column_dimensions['A'].width = 18  # Time Slot
        worksheet.column_dimensions['B'].width = 16  # Temperature
        worksheet.column_dimensions['C'].width = 16  # Dew Point
        worksheet.column_dimensions['D'].width = 14  # Humidity
        worksheet.column_dimensions['E'].width = 18  # Wind Speed
        worksheet.column_dimensions['F'].width = 16  # Pressure
        worksheet.column_dimensions['G'].width = 16  # Precipitation

        # Add title and metadata
        worksheet.insert_rows(1)
        title_cell = worksheet.cell(row=1, column=1)
        title_cell.value = f"Weather Data - {next_day.strftime('%Y-%m-%d')}"
        title_cell.font = Font(bold=True, size=14)
        worksheet.merge_cells('A1:G1')

        # Add generation timestamp and station info
        worksheet.insert_rows(2)
        info_cell = worksheet.cell(row=2, column=1)
        info_cell.value = f"Station: {station_id} | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | Note: Precipitation not interpolated"
        info_cell.font = Font(size=10, italic=True)
        worksheet.merge_cells('A2:G2')

        # Add empty row for spacing
        worksheet.insert_rows(3)

    print(f"✓ Weather Excel file created: {output_path}")
    print(f"✓ Weather date: {next_day}")
    print(f"✓ Time slots: 96 (15-minute intervals)")
    print(f"✓ Station ID: {station_id}")
    print(f"✓ Precipitation handling: Forward-fill only (no interpolation)")

    # Display summary statistics
    print(f"\nWeather Summary:")
    print(f"  Temperature: {weather_15min['temp'].min():.1f}°C to {weather_15min['temp'].max():.1f}°C")
    print(f"  Humidity: {weather_15min['rhum'].min():.1f}% to {weather_15min['rhum'].max():.1f}%")
    print(f"  Wind Speed: {weather_15min['wspd'].min():.1f} to {weather_15min['wspd'].max():.1f} km/h")
    print(f"  Pressure: {weather_15min['pres'].min():.1f} to {weather_15min['pres'].max():.1f} hPa")
    print(f"  Precipitation: {weather_15min['prcp'].min():.1f} to {weather_15min['prcp'].max():.1f} mm")
    print(f"  Zero precipitation intervals: {(weather_15min['prcp'] == 0).sum()}/96")

    return output_path


def display_sample_weather_data(forecast_csv_path="electricity_demand_forecast.csv",
                               station_id='42182'):
    """
    Display a sample of the weather data to verify the conversion
    """
    try:
        forecast_df = pd.read_csv(forecast_csv_path, index_col=0, parse_dates=True)
        first_timestamp = forecast_df.index[0]
        next_day = first_timestamp.date() + timedelta(days=1)

        start_of_day = pd.Timestamp(next_day)
        end_of_day = start_of_day + timedelta(days=1) - timedelta(minutes=15)

        # Create 15-minute time index
        time_index_15min = pd.date_range(
            start=start_of_day,
            end=end_of_day,
            freq='15min'
        )

        # Fetch weather data
        weather_start = start_of_day - timedelta(hours=2)
        weather_end = end_of_day + timedelta(hours=2)

        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres", "prcp"]]

        # Separate precipitation from other variables
        prcp_data = weather_data[['prcp']].copy()
        other_weather = weather_data.drop('prcp', axis=1).copy()

        # Handle non-precipitation variables
        other_weather = other_weather.interpolate(method='linear', limit_direction='both')

        # Handle precipitation (no interpolation)
        prcp_data = prcp_data.fillna(method='ffill').fillna(0.0)

        # Reindex to 15-minute frequency
        other_weather_15min = other_weather.reindex(time_index_15min)
        prcp_15min = prcp_data.reindex(time_index_15min)

        # Apply different strategies
        other_weather_15min = other_weather_15min.interpolate(method='linear', limit_direction='both')
        other_weather_15min = other_weather_15min.bfill().ffill()

        prcp_15min = prcp_15min.fillna(method='ffill').fillna(0.0)

        # Combine back
        weather_15min = pd.concat([other_weather_15min, prcp_15min], axis=1)

        print(f"\nSample weather data for {next_day} (Station: {station_id}):")
        print("=" * 80)
        print("Note: Precipitation values are forward-filled, not interpolated")
        print(weather_15min.head(12).round(1))  # Show first 3 hours (12 intervals)
        print("...")
        print(weather_15min.tail(12).round(1))   # Show last 3 hours (12 intervals)

        return True
    except Exception as e:
        print(f"Could not load weather data: {e}")
        return False


def create_both_excel_files(forecast_csv_path="electricity_demand_forecast.csv",
                           electricity_output="Daily_Electricity_Demand_Forecast.xlsx",
                           weather_output="Daily_Weather_Forecast.xlsx",
                           station_id='42182'):
    """
    Create both electricity and weather Excel files for the same 24-hour period
    """
    print("=" * 80)
    print("CREATING BOTH ELECTRICITY AND WEATHER EXCEL FILES")
    print("=" * 80)

    try:
        # Create electricity Excel (using the existing function from your second script)
        print("\n1. Creating Electricity Demand Forecast Excel...")
        from your_electricity_excel_script import create_forecast_excel  # You'll need to import this
        electricity_file = create_forecast_excel(forecast_csv_path, electricity_output)

        print("\n2. Creating Weather Data Excel...")
        weather_file = create_weather_excel(forecast_csv_path, weather_output, station_id)

        print("\n" + "=" * 80)
        print("BOTH EXCEL FILES CREATED SUCCESSFULLY!")
        print("=" * 80)
        print(f"Electricity forecast: {electricity_file}")
        print(f"Weather data: {weather_file}")
        print(f"Both files cover the same 24-hour period with 15-minute intervals")
        print(f"Note: Precipitation data preserved without interpolation")

        return electricity_file, weather_file

    except Exception as e:
        print(f"Error creating Excel files: {e}")
        raise


if __name__ == "__main__":
    print("=" * 80)
    print("WEATHER DATA EXCEL GENERATOR")
    print("=" * 80)
    print("Note: Precipitation values are forward-filled to preserve zeros")

    # First, try to display sample weather data
    if display_sample_weather_data():
        # Create the weather Excel file
        output_file = create_weather_excel()

        print("\n" + "=" * 80)
        print("WEATHER EXCEL GENERATION COMPLETED!")
        print("=" * 80)
        print(f"Output file: {output_file}")
        print("\nThe Excel file contains:")
        print("• Time slots in 15-minute intervals (00:00-00:15 to 23:45-24:00)")
        print("• Weather data for the same day as electricity forecast")
        print("• Temperature, Dew Point, Humidity, Wind Speed, and Pressure")
        print("• Precipitation data (forward-filled, not interpolated)")
        print("• Professional formatting with headers and borders")
        print("• Summary statistics in the console output")

        print(f"\nTo create both files together, run:")
        print(f"create_both_excel_files()")

    else:
        print("\nError: Could not fetch weather data.")
        print("Please check:")
        print("• Internet connection")
        print("• Meteostat API availability")
        print("• Station ID validity (current: '42182')")
        print("• Ensure 'electricity_demand_forecast.csv' exists")

import pandas as pd
from datetime import datetime, timedelta
import openpyxl

# === CONFIGURATION ===
excel_path = 'Daily_Electricity_Demand_Forecast.xlsx'  # Your Excel file path

def extract_date_from_excel(excel_path):
    """Extract the forecast date from the Excel file title"""
    try:
        workbook = openpyxl.load_workbook(excel_path)
        worksheet = workbook.active

        # Get the title cell (should be in A1 after the formatting)
        title_cell = worksheet['A1'].value
        if title_cell and "Electricity Demand Forecast -" in str(title_cell):
            date_str = str(title_cell).split("- ")[-1]
            forecast_date = datetime.strptime(date_str, '%Y-%m-%d').date()
            return forecast_date
        else:
            print("Could not extract date from Excel title, using today + 1")
            return datetime.now().date() + timedelta(days=1)
    except Exception as e:
        print(f"Error extracting date: {e}, using today + 1")
        return datetime.now().date() + timedelta(days=1)

def parse_time_slot(time_slot_str):
    """Convert time slot string to datetime"""
    # Extract start time from "HH:MM - HH:MM" format
    start_time_str = time_slot_str.split(" - ")[0]
    hour, minute = map(int, start_time_str.split(":"))
    return hour, minute

# === PREPARE DATA ===
print("Reading Excel file...")

# Read Excel file, skipping the title and metadata rows
df = pd.read_excel(excel_path, sheet_name='Daily Forecast', skiprows=3)

print(f"Loaded {len(df)} rows from Excel")
print(f"Columns: {list(df.columns)}")

# Get the forecast date
forecast_date = extract_date_from_excel(excel_path)
print(f"Forecast date: {forecast_date}")

# Convert to Supabase format
supabase_data = []

for idx, row in df.iterrows():
    try:
        time_slot = row['Time Slot']
        demand_forecast = row['Demand Forecast (MW)']

        # Skip if demand forecast is null/empty
        if pd.isna(demand_forecast):
            continue

        # Parse time slot to get hour and minute
        hour, minute = parse_time_slot(time_slot)

        # Create datetime
        dt = datetime.combine(forecast_date, datetime.min.time().replace(hour=hour, minute=minute))

        supabase_data.append({
            'datetime': dt,
            'da_forecast': float(demand_forecast)
        })

    except Exception as e:
        print(f"Error processing row {idx}: {e}")
        continue

# Create DataFrame for Supabase
df_supa = pd.DataFrame(supabase_data)


from supabase import create_client, Client

# === SUPABASE CONFIGURATION ===
supabase_url = 'https://ikuvbtsxjemnkfbgcjrl.supabase.co'
supabase_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImlrdXZidHN4amVtbmtmYmdjanJsIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTI4NzE5NjIsImV4cCI6MjA2ODQ0Nzk2Mn0.h7ofTtHrJx40A-SUP-f3v-6CbQd-eauFgx4Bhxg5Wtc'
supabase_table = "da-forecast"

# === CONNECT TO SUPABASE ===
supabase: Client = create_client(supabase_url, supabase_key)

# === CONVERT datetime to string ===
df_supa['datetime'] = df_supa['datetime'].astype(str)  # OR use .dt.strftime('%Y-%m-%d %H:%M:%S') if you want more control

# === PUSH TO SUPABASE ===
records = df_supa.to_dict(orient='records')

print(f"Pushing {len(records)} records to Supabase...")

for record in records:
    try:
        supabase.table(supabase_table).upsert(record).execute()
    except Exception as e:
        print(f"Failed to insert record {record}: {e}")

print("✅ Upload complete.")

"""## Archive"""

import pandas as pd
from datetime import datetime, timedelta
import openpyxl

# === CONFIGURATION ===
excel_path = 'Daily_Electricity_Demand_Forecast_29July.xlsx'  # Your Excel file path

def extract_date_from_excel(excel_path):
    """Extract the forecast date from the Excel file title"""
    try:
        workbook = openpyxl.load_workbook(excel_path)
        worksheet = workbook.active

        # Get the title cell (should be in A1 after the formatting)
        title_cell = worksheet['A1'].value
        if title_cell and "Electricity Demand Forecast -" in str(title_cell):
            date_str = str(title_cell).split("- ")[-1]
            forecast_date = datetime.strptime(date_str, '%Y-%m-%d').date()
            return forecast_date
        else:
            print("Could not extract date from Excel title, using today + 1")
            return datetime.now().date() + timedelta(days=1)
    except Exception as e:
        print(f"Error extracting date: {e}, using today + 1")
        return datetime.now().date() + timedelta(days=1)

def parse_time_slot(time_slot_str):
    """Convert time slot string to datetime"""
    # Extract start time from "HH:MM - HH:MM" format
    start_time_str = time_slot_str.split(" - ")[0]
    hour, minute = map(int, start_time_str.split(":"))
    return hour, minute

# === PREPARE DATA ===
print("Reading Excel file...")

# Read Excel file, skipping the title and metadata rows
df = pd.read_excel(excel_path, sheet_name='Daily Forecast', skiprows=3)

print(f"Loaded {len(df)} rows from Excel")
print(f"Columns: {list(df.columns)}")

# Get the forecast date
forecast_date = extract_date_from_excel(excel_path)
print(f"Forecast date: {forecast_date}")

# Convert to Supabase format
supabase_data = []

for idx, row in df.iterrows():
    try:
        time_slot = row['Time Slot']
        demand_forecast = row['Demand Forecast (MW)']

        # Skip if demand forecast is null/empty
        if pd.isna(demand_forecast):
            continue

        # Parse time slot to get hour and minute
        hour, minute = parse_time_slot(time_slot)

        # Create datetime
        dt = datetime.combine(forecast_date, datetime.min.time().replace(hour=hour, minute=minute))

        supabase_data.append({
            'datetime': dt,
            'da_forecast': float(demand_forecast)
        })

    except Exception as e:
        print(f"Error processing row {idx}: {e}")
        continue

# Create DataFrame for Supabase
df_supa = pd.DataFrame(supabase_data)

df_supa

import requests
import pandas as pd

# === CONFIGURATION ===
supabase_url = 'https://ikuvbtsxjemnkfbgcjrl.supabase.co'
supabase_table = "da-forecast"  # Your table name
supabase_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImlrdXZidHN4amVtbmtmYmdjanJsIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTI4NzE5NjIsImV4cCI6MjA2ODQ0Nzk2Mn0.h7ofTtHrJx40A-SUP-f3v-6CbQd-eauFgx4Bhxg5Wtc'

def upload_to_supabase(df_supa):
    """Upload DataFrame to Supabase table"""

    # Convert DataFrame to records format for Supabase
    print("Converting DataFrame to Supabase format...")

    # Convert datetime to string format for Supabase
    df_upload = df_supa.copy()
    df_upload['datetime'] = df_upload['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')

    # Convert to list of dictionaries
    records = df_upload.to_dict('records')

    print(f"Prepared {len(records)} records for upload")

    # Setup headers
    headers = {
        'apikey': supabase_key,
        'Authorization': f'Bearer {supabase_key}',
        'Content-Type': 'application/json',
        'Prefer': 'resolution=merge-duplicates'
    }

    # Use quoted table name to handle hyphens
    insert_url = f'{supabase_url}/rest/v1/"{supabase_table}"?on_conflict=datetime'

    print(f"Uploading to: {insert_url}")

    # Upload in batches
    batch_size = 100
    successful_batches = 0
    failed_batches = 0
    total_uploaded = 0

    print(f"\nStarting upload in batches of {batch_size}...")

    for i in range(0, len(records), batch_size):
        batch = records[i:i+batch_size]
        batch_num = i//batch_size + 1

        try:
            response = requests.post(insert_url, headers=headers, json=batch, timeout=30)

            if response.status_code in [200, 201, 204]:
                print(f"✓ Batch {batch_num} ({len(batch)} records) uploaded successfully")
                successful_batches += 1
                total_uploaded += len(batch)
            else:
                print(f"✗ Error uploading batch {batch_num}: HTTP {response.status_code}")
                print(f"  Response: {response.text[:200]}...")
                failed_batches += 1

        except requests.exceptions.RequestException as e:
            print(f"✗ Network error for batch {batch_num}: {str(e)}")
            failed_batches += 1
        except Exception as e:
            print(f"✗ Unexpected error for batch {batch_num}: {str(e)}")
            failed_batches += 1

    # Upload summary
    print(f"\n=== UPLOAD SUMMARY ===")
    print(f"Total records: {len(records)}")
    print(f"Successfully uploaded: {total_uploaded}")
    print(f"Successful batches: {successful_batches}")
    print(f"Failed batches: {failed_batches}")

    return failed_batches == 0, total_uploaded

def verify_upload(df_supa):
    """Verify that data was uploaded correctly"""
    headers = {
        'apikey': supabase_key,
        'Authorization': f'Bearer {supabase_key}',
    }

    # Get first and last datetime for verification
    first_datetime = df_supa['datetime'].min().strftime('%Y-%m-%d %H:%M:%S')
    last_datetime = df_supa['datetime'].max().strftime('%Y-%m-%d %H:%M:%S')

    print(f"\n=== VERIFICATION ===")

    # Check first record
    verify_url = f'{supabase_url}/rest/v1/"{supabase_table}"?datetime=eq.{first_datetime}&select=*'

    try:
        response = requests.get(verify_url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data:
                print(f"✓ First record verified: {data[0]}")
            else:
                print(f"✗ First record not found: {first_datetime}")
        else:
            print(f"✗ Verification failed: HTTP {response.status_code}")
    except Exception as e:
        print(f"✗ Verification error: {str(e)}")

    # Get count of uploaded records
    count_url = f'{supabase_url}/rest/v1/"{supabase_table}"?select=count&datetime=gte.{first_datetime}&datetime=lte.{last_datetime}'
    count_headers = headers.copy()
    count_headers['Prefer'] = 'count=exact'

    try:
        response = requests.get(count_url, headers=count_headers, timeout=10)
        if response.status_code == 200:
            content_range = response.headers.get('Content-Range', '')
            if content_range:
                total_count = content_range.split('/')[-1]
                print(f"✓ Total records in Supabase for this forecast: {total_count}")
        else:
            print(f"✗ Count verification failed: HTTP {response.status_code}")
    except Exception as e:
        print(f"✗ Count verification error: {str(e)}")

# === MAIN UPLOAD EXECUTION ===
if 'df_supa' not in locals():
    print("❌ ERROR: df_supa not found!")
    print("Please run the data preparation cell first.")
else:
    print(f"📊 Found df_supa with {len(df_supa)} records")
    print(f"📅 Date range: {df_supa['datetime'].min()} to {df_supa['datetime'].max()}")

    # Confirm before upload
    print(f"\n🚀 Ready to upload {len(df_supa)} records to Supabase table '{supabase_table}'")

    # Upload the data
    success, uploaded_count = upload_to_supabase(df_supa)

    if success:
        print(f"\n🎉 SUCCESS! Uploaded {uploaded_count} records")
        verify_upload(df_supa)
    else:
        print(f"\n⚠️  Upload completed with some errors")

    print(f"\n{'='*60}")
    print("UPLOAD PROCESS COMPLETED")
    print(f"{'='*60}")

"""## Archive"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import openpyxl
from openpyxl.styles import Font, Alignment, Border, Side, PatternFill
from meteostat import Hourly

def create_weather_excel(forecast_csv_path="electricity_demand_forecast.csv",
                        output_path="Daily_Weather_Forecast.xlsx",
                        station_id='42182'):
    """
    Create an Excel file with weather data for the same 24-hour period as the electricity forecast.
    Uses 15-minute intervals to match the electricity demand forecast.
    """

    print("Loading forecast data to determine date range...")

    # Load the forecast data to get the date range
    try:
        forecast_df = pd.read_csv(forecast_csv_path, index_col=0, parse_dates=True)
        first_timestamp = forecast_df.index[0]
        next_day = first_timestamp.date() + timedelta(days=1)

        print(f"Target forecast date: {next_day}")
    except Exception as e:
        print(f"Error loading forecast data: {e}")
        print("Using current date + 1 as fallback...")
        next_day = datetime.now().date() + timedelta(days=1)

    # Define the 24-hour period (96 fifteen-minute intervals)
    start_of_day = pd.Timestamp(next_day)
    end_of_day = start_of_day + timedelta(days=1) - timedelta(minutes=15)

    print(f"Fetching weather data for: {start_of_day} to {end_of_day}")

    # Create 15-minute time index
    time_index_15min = pd.date_range(
        start=start_of_day,
        end=end_of_day,
        freq='15min'
    )

    # Fetch hourly weather data with some buffer
    weather_start = start_of_day - timedelta(hours=2)
    weather_end = end_of_day + timedelta(hours=2)

    try:
        print(f"Fetching weather data from station {station_id}...")
        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres", "prcp"]]

        print(f"Raw weather data shape: {weather_data.shape}")

        # Fill missing values at hourly level
        weather_data = weather_data.interpolate(method='linear', limit_direction='both')

        # Reindex to 15-minute frequency and interpolate
        weather_15min = weather_data.reindex(time_index_15min)
        weather_15min = weather_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values
        weather_15min = weather_15min.bfill().ffill()

        print(f"15-minute weather data shape: {weather_15min.shape}")
        print(f"Missing values: {weather_15min.isnull().sum().sum()}")

    except Exception as e:
        print(f"✗ Error fetching weather data: {e}")
        print("Unable to proceed without valid weather data from the API.")
        raise Exception("Weather data fetch failed. Please check your internet connection and try again.")

    # Create time slot labels (same as electricity forecast)
    time_slots = []
    for hour in range(24):
        for minute in [0, 15, 30, 45]:
            start_time = f"{hour:02d}:{minute:02d}"
            end_minute = minute + 15
            if end_minute == 60:
                end_hour = hour + 1
                end_minute = 0
            else:
                end_hour = hour

            if end_hour == 24:
                end_time = "24:00"
            else:
                end_time = f"{end_hour:02d}:{end_minute:02d}"

            time_slots.append(f"{start_time} - {end_time}")

    # Prepare Excel data
    excel_data = {
        'Time Slot': time_slots,
        'Temperature (°C)': [round(val, 1) for val in weather_15min['temp'].values],
        'Dew Point (°C)': [round(val, 1) for val in weather_15min['dwpt'].values],
        'Humidity (%)': [round(val, 1) for val in weather_15min['rhum'].values],
        'Wind Speed (km/h)': [round(val, 1) for val in weather_15min['wspd'].values],
        'Pressure (hPa)': [round(val, 1) for val in weather_15min['pres'].values],
        'Precipitation (mm)': [round(val, 1) for val in weather_15min['prcp'].values]
    }

    # Create DataFrame
    result_df = pd.DataFrame(excel_data)

    # Create Excel file with formatting
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        result_df.to_excel(writer, sheet_name='Daily Weather', index=False)

        # Get the workbook and worksheet
        workbook = writer.book
        worksheet = writer.sheets['Daily Weather']

        # Define styles
        header_font = Font(bold=True, size=12, color="FFFFFF")
        header_fill = PatternFill(start_color="2E8B57", end_color="2E8B57", fill_type="solid")  # Sea green
        data_font = Font(size=10)
        border = Border(
            left=Side(border_style="thin"),
            right=Side(border_style="thin"),
            top=Side(border_style="thin"),
            bottom=Side(border_style="thin")
        )
        center_alignment = Alignment(horizontal="center", vertical="center")

        # Apply header formatting
        for col in range(1, 8):  # Columns A through F
            cell = worksheet.cell(row=1, column=col)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = center_alignment
            cell.border = border

        # Apply data formatting
        for row in range(2, 98):  # Rows 2 to 97 (96 time slots + header)
            for col in range(1, 7):
                cell = worksheet.cell(row=row, column=col)
                cell.font = data_font
                cell.border = border
                if col == 1:  # Time slot column
                    cell.alignment = center_alignment
                else:  # Weather data columns
                    cell.alignment = Alignment(horizontal="right", vertical="center")

        # Adjust column widths
        worksheet.column_dimensions['A'].width = 18  # Time Slot
        worksheet.column_dimensions['B'].width = 16  # Temperature
        worksheet.column_dimensions['C'].width = 16  # Dew Point
        worksheet.column_dimensions['D'].width = 14  # Humidity
        worksheet.column_dimensions['E'].width = 18  # Wind Speed
        worksheet.column_dimensions['F'].width = 16  # Pressure
        worksheet.column_dimensions['G'].width = 16  # Precipitation

        # Add title and metadata
        worksheet.insert_rows(1)
        title_cell = worksheet.cell(row=1, column=1)
        title_cell.value = f"Weather Data - {next_day.strftime('%Y-%m-%d')}"
        title_cell.font = Font(bold=True, size=14)
        worksheet.merge_cells('A1:G1')

        # Add generation timestamp and station info
        worksheet.insert_rows(2)
        info_cell = worksheet.cell(row=2, column=1)
        info_cell.value = f"Station: {station_id} | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        info_cell.font = Font(size=10, italic=True)
        worksheet.merge_cells('A2:G2')

        # Add empty row for spacing
        worksheet.insert_rows(3)

    print(f"✓ Weather Excel file created: {output_path}")
    print(f"✓ Weather date: {next_day}")
    print(f"✓ Time slots: 96 (15-minute intervals)")
    print(f"✓ Station ID: {station_id}")

    # Display summary statistics
    print(f"\nWeather Summary:")
    print(f"  Temperature: {weather_15min['temp'].min():.1f}°C to {weather_15min['temp'].max():.1f}°C")
    print(f"  Humidity: {weather_15min['rhum'].min():.1f}% to {weather_15min['rhum'].max():.1f}%")
    print(f"  Wind Speed: {weather_15min['wspd'].min():.1f} to {weather_15min['wspd'].max():.1f} km/h")
    print(f"  Pressure: {weather_15min['pres'].min():.1f} to {weather_15min['pres'].max():.1f} hPa")
    print(f"  Precipitation: {weather_15min['prcp'].min():.1f} to {weather_15min['prcp'].max():.1f} mm")

    return output_path


def display_sample_weather_data(forecast_csv_path="electricity_demand_forecast.csv",
                               station_id='42182'):
    """
    Display a sample of the weather data to verify the conversion
    """
    try:
        forecast_df = pd.read_csv(forecast_csv_path, index_col=0, parse_dates=True)
        first_timestamp = forecast_df.index[0]
        next_day = first_timestamp.date() + timedelta(days=1)

        start_of_day = pd.Timestamp(next_day)
        end_of_day = start_of_day + timedelta(days=1) - timedelta(minutes=15)

        # Create 15-minute time index
        time_index_15min = pd.date_range(
            start=start_of_day,
            end=end_of_day,
            freq='15min'
        )

        # Fetch weather data
        weather_start = start_of_day - timedelta(hours=2)
        weather_end = end_of_day + timedelta(hours=2)

        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres", "prcp"]]
        weather_data = weather_data.interpolate(method='linear', limit_direction='both')

        weather_15min = weather_data.reindex(time_index_15min)
        weather_15min = weather_15min.interpolate(method='linear', limit_direction='both')
        weather_15min = weather_15min.bfill().ffill()

        print(f"\nSample weather data for {next_day} (Station: {station_id}):")
        print("=" * 80)
        print(weather_15min.head(12).round(1))  # Show first 3 hours (12 intervals)
        print("...")
        print(weather_15min.tail(12).round(1))   # Show last 3 hours (12 intervals)

        return True
    except Exception as e:
        print(f"Could not load weather data: {e}")
        return False


def create_both_excel_files(forecast_csv_path="electricity_demand_forecast.csv",
                           electricity_output="Daily_Electricity_Demand_Forecast.xlsx",
                           weather_output="Daily_Weather_Forecast.xlsx",
                           station_id='42182'):
    """
    Create both electricity and weather Excel files for the same 24-hour period
    """
    print("=" * 80)
    print("CREATING BOTH ELECTRICITY AND WEATHER EXCEL FILES")
    print("=" * 80)

    try:
        # Create electricity Excel (using the existing function from your second script)
        print("\n1. Creating Electricity Demand Forecast Excel...")
        from your_electricity_excel_script import create_forecast_excel  # You'll need to import this
        electricity_file = create_forecast_excel(forecast_csv_path, electricity_output)

        print("\n2. Creating Weather Data Excel...")
        weather_file = create_weather_excel(forecast_csv_path, weather_output, station_id)

        print("\n" + "=" * 80)
        print("BOTH EXCEL FILES CREATED SUCCESSFULLY!")
        print("=" * 80)
        print(f"Electricity forecast: {electricity_file}")
        print(f"Weather data: {weather_file}")
        print(f"Both files cover the same 24-hour period with 15-minute intervals")

        return electricity_file, weather_file

    except Exception as e:
        print(f"Error creating Excel files: {e}")
        raise


if __name__ == "__main__":
    print("=" * 80)
    print("WEATHER DATA EXCEL GENERATOR")
    print("=" * 80)

    # First, try to display sample weather data
    if display_sample_weather_data():
        # Create the weather Excel file
        output_file = create_weather_excel()

        print("\n" + "=" * 80)
        print("WEATHER EXCEL GENERATION COMPLETED!")
        print("=" * 80)
        print(f"Output file: {output_file}")
        print("\nThe Excel file contains:")
        print("• Time slots in 15-minute intervals (00:00-00:15 to 23:45-24:00)")
        print("• Weather data for the same day as electricity forecast")
        print("• Temperature, Dew Point, Humidity, Wind Speed, and Pressure")
        print("• Professional formatting with headers and borders")
        print("• Summary statistics in the console output")

        print(f"\nTo create both files together, run:")
        print(f"create_both_excel_files()")

    else:
        print("\nError: Could not fetch weather data.")
        print("Please check:")
        print("• Internet connection")
        print("• Meteostat API availability")
        print("• Station ID validity (current: '42182')")
        print("• Ensure 'electricity_demand_forecast.csv' exists")

"""## CAISO demand forecast"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import openpyxl
from openpyxl.styles import Font, Alignment, Border, Side, PatternFill

df = pd.read_excel('/content/CISO (3).xlsx')

columns_to_keep = ['UTC time','Demand','Demand forecast']
df = df[columns_to_keep]

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Convert UTC time to datetime if it's not already
df['UTC time'] = pd.to_datetime(df['UTC time'])

# Method 1: Using plotly.express (simpler approach)
fig = px.line(df, x='UTC time', y=['Demand', 'Demand forecast'],
              title='UTC Time vs Demand vs Demand Forecast',
              labels={'value': 'Value', 'variable': 'Type'})

# Customize the plot
fig.update_layout(
    xaxis_title='UTC Time',
    yaxis_title='Demand/Forecast Value',
    hovermode='x unified',
    width=900,
    height=600
)

# Show the plot
fig.show()


import pandas as pd
from darts import TimeSeries
from darts.dataprocessing.transformers import MissingValuesFiller

df = pd.read_excel('/content/CISO (3).xlsx')

columns_to_keep = ['UTC time','Demand','Demand forecast']
df = df[columns_to_keep]

demand_series = TimeSeries.from_dataframe(df, value_cols='Demand',
                                          freq='H')

filler = MissingValuesFiller()
interpolated_series = filler.transform(demand_series)

interpolated_df = interpolated_series.to_dataframe()

# === Imports ===
import pandas as pd
import numpy as np
from datetime import datetime
from darts import TimeSeries, concatenate
from darts.models import TiDEModel
from darts.metrics import mape, mae, rmse
from darts.dataprocessing.transformers import Scaler
from sklearn.preprocessing import MinMaxScaler
from pytorch_lightning.callbacks import EarlyStopping
import torch
import torch.nn as nn
import holidays
import warnings

warnings.filterwarnings("ignore")

# === Custom Loss Function ===
class CustomLast24hMAELoss(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, y_pred, y_true):
        # For hourly data, last 24h = 24 time steps
        return torch.mean(torch.abs(y_true[:, -24:, :] - y_pred[:, -24:, :]))

# === Calendar Features ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add US holidays for CAISO (California)
    us_holidays = holidays.country_holidays("US", state="CA", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(us_holidays.keys()))).astype(int)

    # Add season features
    features["season"] = ((index.month % 12 + 3) // 3).astype(int)  # 1=Winter, 2=Spring, 3=Summer, 4=Fall
    features["is_summer"] = (features["season"] == 3).astype(int)  # Peak demand season
    features["is_winter"] = (features["season"] == 1).astype(int)

    # Add working hours feature
    features["is_working_hours"] = ((index.hour >= 8) & (index.hour <= 18) &
                                   (index.dayofweek < 5) &
                                   (features["is_holiday"] == 0)).astype(int)

    return features

df_processed = interpolated_df.copy()
df_processed.columns = df_processed.columns.str.strip()

# Convert datetime and set as index
df_processed = df_processed.sort_index()

# Keep only the target variable (demand)
df_processed = df_processed[['Demand']]

print(f"CAISO demand data shape: {df_processed.shape}")
print(f"Data range: {df_processed.index.min()} to {df_processed.index.max()}")
print(f"Data frequency: {pd.infer_freq(df_processed.index)}")

# === Create Calendar Features as Future Covariates ===
calendar_df = create_calendar_features(df_processed.index)

print(f"Calendar features created: {calendar_df.shape[1]}")
print("Calendar features include:")
for col in calendar_df.columns:
    print(f"  - {col}")

# === Create Darts TimeSeries ===
target_ts = TimeSeries.from_dataframe(df_processed, value_cols=["Demand"])
future_covariates_ts = TimeSeries.from_dataframe(calendar_df)

print(f"Target time series length: {len(target_ts)}")
print(f"Future covariates length: {len(future_covariates_ts)}")

# === Train/Validation/Test Split ===
# Adjust these dates based on your CAISO data range
# Using 70% for training, 15% for validation, 15% for testing

total_length = len(target_ts)
train_length = int(0.7 * total_length)
val_length = int(0.15 * total_length)

train_end_idx = train_length
val_end_idx = train_length + val_length

# Get actual timestamps for splits
train_end = target_ts.time_index[train_end_idx - 1]
val_end = target_ts.time_index[val_end_idx - 1]

# Target series splits
train_ts = target_ts[:train_end_idx]
val_ts = target_ts[train_end_idx:val_end_idx]
test_ts = target_ts[val_end_idx:]

# Future covariates splits
train_future_cov = future_covariates_ts[:train_end_idx]
val_future_cov = future_covariates_ts[train_end_idx:val_end_idx]
test_future_cov = future_covariates_ts[val_end_idx:]

# Verification
print(f"\nData splits:")
print(f"Training data: {train_ts.start_time()} to {train_ts.end_time()} ({len(train_ts)} points)")
print(f"Validation data: {val_ts.start_time()} to {val_ts.end_time()} ({len(val_ts)} points)")
print(f"Test data: {test_ts.start_time()} to {test_ts.end_time()} ({len(test_ts)} points)")

# === Scale Target and Covariates ===
scaler_y = Scaler(scaler=MinMaxScaler())
scaler_future_cov = Scaler(scaler=MinMaxScaler())

# Scale target
train_y = scaler_y.fit_transform(train_ts).astype(np.float32)
val_y = scaler_y.transform(val_ts).astype(np.float32)
test_y = scaler_y.transform(test_ts).astype(np.float32)

# Scale future covariates (calendar features only)
train_future_cov = scaler_future_cov.fit_transform(train_future_cov).astype(np.float32)
val_future_cov = scaler_future_cov.transform(val_future_cov).astype(np.float32)
test_future_cov = scaler_future_cov.transform(test_future_cov).astype(np.float32)

print(f"\nScaled data shapes:")
print(f"Target train: {train_y.values().shape}")
print(f"Future covariates train: {train_future_cov.values().shape}")

# === Configure TiDEModel ===
# Adjust parameters for hourly data (vs 15-minute in original)
model = TiDEModel(
    input_chunk_length=168,      # 7 days of hourly input (7*24)
    output_chunk_length=48,      # 24 hours of output (1 day forecast)
    hidden_size=1024,             # Reduced from 1024 for simpler calendar-only features
    temporal_decoder_hidden=32,
    temporal_width_past=4,
    temporal_width_future=4,
    num_encoder_layers=5,
    num_decoder_layers=5,
    decoder_output_dim=32,
    use_layer_norm=False,
    dropout=0.2,
    n_epochs=100,
    batch_size=32,
    optimizer_kwargs={"lr": 1e-3},
    loss_fn=CustomLast24hMAELoss(),
    pl_trainer_kwargs={
        "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
        "devices": 1,
        "precision": "32-true",
        "gradient_clip_val": 1.0,
        "callbacks": [EarlyStopping(monitor="val_loss", patience=15)]
    },
    save_checkpoints=True,
    force_reset=True,
    random_state=42
)

# === Train the Model ===
print("\nTraining TiDE model with calendar features only...")
print(f"Input chunk length: {model.input_chunk_length} hours ({model.input_chunk_length // 24} days)")
print(f"Output chunk length: {model.output_chunk_length} hours ({model.output_chunk_length // 24} days)")
print(f"Future covariates: {calendar_df.shape[1]} calendar features")
print(f"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")

model.fit(
    series=train_y,
    future_covariates=train_future_cov,
    val_series=val_y,
    val_future_covariates=val_future_cov,
    verbose=True
)

# === Generate Predictions ===
print("\nGenerating predictions...")

"""## v2 for CAISO"""

# === Imports ===
import pandas as pd
import numpy as np
from datetime import datetime
from darts import TimeSeries, concatenate
from darts.models import TiDEModel
from darts.metrics import mape, mae, rmse
from darts.dataprocessing.transformers import Scaler, MissingValuesFiller
from sklearn.preprocessing import MinMaxScaler
from pytorch_lightning.callbacks import EarlyStopping
import torch
import torch.nn as nn
import holidays
import warnings
import plotly.graph_objects as go
import plotly.express as px

warnings.filterwarnings("ignore")

# === Data Loading and Preprocessing ===
df = pd.read_excel('/content/CISO (3).xlsx')

columns_to_keep = ['UTC time','Demand','Demand forecast']
df = df[columns_to_keep]

# Convert to datetime and remove 2025 data
df['UTC time'] = pd.to_datetime(df['UTC time'])
df = df[df['UTC time'].dt.year < 2025]  # Remove 2025 data

print(f"Data range after removing 2025: {df['UTC time'].min()} to {df['UTC time'].max()}")
print(f"Original data shape: {df.shape}")
print(f"Missing values in Demand: {df['Demand'].isnull().sum()}")

# Create TimeSeries for interpolation
demand_series = TimeSeries.from_dataframe(df, time_col='UTC time', value_cols='Demand', freq='H')

# Fill missing values using Darts
filler = MissingValuesFiller(fill='auto')  # Uses interpolation
interpolated_series = filler.transform(demand_series)

# Convert back to DataFrame
interpolated_df = interpolated_series.to_dataframe()
interpolated_df.columns = ['Demand']

print(f"Data shape after interpolation: {interpolated_df.shape}")
print(f"Missing values after interpolation: {interpolated_df['Demand'].isnull().sum()}")

# === Custom Loss Function ===
class CustomLast24hMAELoss(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, y_pred, y_true):
        # For 48-hour forecast, focus on last 24 hours (indices 24:48)
        return torch.mean(torch.abs(y_true[:, -24:, :] - y_pred[:, -24:, :]))

# === Calendar Features ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add US holidays for CAISO (California)
    us_holidays = holidays.country_holidays("US", state="CA", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(us_holidays.keys()))).astype(int)

    # Add season features
    features["season"] = ((index.month % 12 + 3) // 3).astype(int)  # 1=Winter, 2=Spring, 3=Summer, 4=Fall
    features["is_summer"] = (features["season"] == 3).astype(int)  # Peak demand season
    features["is_winter"] = (features["season"] == 1).astype(int)

    # Add working hours feature
    features["is_working_hours"] = ((index.hour >= 8) & (index.hour <= 18) &
                                   (index.dayofweek < 5) &
                                   (features["is_holiday"] == 0)).astype(int)

    return features

# === Prepare Data ===
df_processed = interpolated_df.copy()
df_processed.columns = df_processed.columns.str.strip()
df_processed = df_processed.sort_index()

print(f"CAISO demand data shape: {df_processed.shape}")
print(f"Data range: {df_processed.index.min()} to {df_processed.index.max()}")
print(f"Data frequency: {pd.infer_freq(df_processed.index)}")

# === Create Calendar Features as Future Covariates ===
calendar_df = create_calendar_features(df_processed.index)

print(f"Calendar features created: {calendar_df.shape[1]}")
print("Calendar features include:")
for col in calendar_df.columns:
    print(f"  - {col}")

# === Create Darts TimeSeries ===
target_ts = TimeSeries.from_dataframe(df_processed, value_cols=["Demand"])
future_covariates_ts = TimeSeries.from_dataframe(calendar_df)

print(f"Target time series length: {len(target_ts)}")
print(f"Future covariates length: {len(future_covariates_ts)}")

# === Train/Validation/Test Split ===
total_length = len(target_ts)
train_length = int(0.7 * total_length)
val_length = int(0.15 * total_length)

train_end_idx = train_length
val_end_idx = train_length + val_length

# Target series splits
train_ts = target_ts[:train_end_idx]
val_ts = target_ts[train_end_idx:val_end_idx]
test_ts = target_ts[val_end_idx:]

# Future covariates splits
train_future_cov = future_covariates_ts[:train_end_idx]
val_future_cov = future_covariates_ts[train_end_idx:val_end_idx]
test_future_cov = future_covariates_ts[val_end_idx:]

# Verification
print(f"\nData splits:")
print(f"Training data: {train_ts.start_time()} to {train_ts.end_time()} ({len(train_ts)} points)")
print(f"Validation data: {val_ts.start_time()} to {val_ts.end_time()} ({len(val_ts)} points)")
print(f"Test data: {test_ts.start_time()} to {test_ts.end_time()} ({len(test_ts)} points)")

# === Scale Target and Covariates ===
scaler_y = Scaler(scaler=MinMaxScaler())
scaler_future_cov = Scaler(scaler=MinMaxScaler())

# Scale target
train_y = scaler_y.fit_transform(train_ts).astype(np.float32)
val_y = scaler_y.transform(val_ts).astype(np.float32)
test_y = scaler_y.transform(test_ts).astype(np.float32)

# Scale future covariates (calendar features only)
train_future_cov = scaler_future_cov.fit_transform(train_future_cov).astype(np.float32)
val_future_cov = scaler_future_cov.transform(val_future_cov).astype(np.float32)
test_future_cov = scaler_future_cov.transform(test_future_cov).astype(np.float32)

print(f"\nScaled data shapes:")
print(f"Target train: {train_y.values().shape}")
print(f"Future covariates train: {train_future_cov.values().shape}")

# === Configure TiDEModel ===
model = TiDEModel(
    input_chunk_length=168,      # 7 days of hourly input (7*24)
    output_chunk_length=48,      # 48 hours of output (focus on last 24)
    hidden_size=1024,
    temporal_decoder_hidden=32,
    temporal_width_past=4,
    temporal_width_future=4,
    num_encoder_layers=5,
    num_decoder_layers=5,
    decoder_output_dim=32,
    use_layer_norm=False,
    dropout=0.2,
    n_epochs=10,
    batch_size=32,
    optimizer_kwargs={"lr": 1e-3},
    loss_fn=CustomLast24hMAELoss(),  # Custom loss focusing on last 24 hours
    pl_trainer_kwargs={
        "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
        "devices": 1,
        "precision": "32-true",
        "gradient_clip_val": 1.0,
        "callbacks": [EarlyStopping(monitor="val_loss", patience=15)]
    },
    save_checkpoints=True,
    force_reset=True,
    random_state=42
)

# === Train the Model ===
print("\nTraining TiDE model with calendar features only...")
print(f"Input chunk length: {model.input_chunk_length} hours ({model.input_chunk_length // 24} days)")
print(f"Output chunk length: {model.output_chunk_length} hours ({model.output_chunk_length // 24} days)")
print(f"Future covariates: {calendar_df.shape[1]} calendar features")
print(f"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
print("Custom loss function focuses on the LAST 24 hours of the 48-hour forecast")

model.fit(
    series=train_y,
    future_covariates=train_future_cov,
    val_series=val_y,
    val_future_covariates=val_future_cov,
    verbose=True
)

print("Training completed!")

# === Load Best Model ===
print("\nLoading best model checkpoint...")
best_model = TiDEModel.load_from_checkpoint("TiDEModel")
print("Best model loaded successfully!")

# === Generate Historical Forecasts ===
print("\nGenerating historical forecasts...")

# Ensure consistent data types
full_series = concatenate([train_y, val_y, test_y], axis=0).astype(np.float32)
future_covariates_full = future_covariates_ts.astype(np.float32)

# Generate historical forecasts on test set
test_historical_forecasts = best_model.historical_forecasts(
    series=full_series,
    future_covariates=future_covariates_full,
    start=len(train_y) + len(val_y),  # Start forecasting from test period
    forecast_horizon=48,  # 48-hour forecasts
    stride=1,  # Generate forecast at each time step
    retrain=False,
    verbose=True
)

# === Inverse Transform Predictions ===
test_historical_forecasts_unscaled = scaler_y.inverse_transform(test_historical_forecasts)
test_y_unscaled = scaler_y.inverse_transform(test_y)

# === Extract Last 24 Hours from 48-Hour Forecasts ===
# For each 48-hour forecast, extract only the last 24 hours (hours 24-47)
# This requires processing each forecast individually
forecast_list = []
actual_list = []
time_list = []

for i in range(len(test_historical_forecasts_unscaled)):
    # Get the 48-hour forecast
    forecast_48h = test_historical_forecasts_unscaled[i]

    # Extract last 24 hours (indices 24:48)
    forecast_last24h = forecast_48h.values()[24:48]  # Last 24 hours

    # Get corresponding actual values (24 hours ahead from forecast start)
    actual_start_idx = i + 24  # 24 hours ahead
    if actual_start_idx + 24 <= len(test_y_unscaled):
        actual_24h = test_y_unscaled.values()[actual_start_idx:actual_start_idx + 24]

        # Get time indices for the actual values
        time_24h = test_y_unscaled.time_index[actual_start_idx:actual_start_idx + 24]

        forecast_list.extend(forecast_last24h.flatten())
        actual_list.extend(actual_24h.flatten())
        time_list.extend(time_24h)

# Create final aligned series
test_forecasts_final = np.array(forecast_list)
test_actual_final = np.array(actual_list)
time_final = time_list

# === Evaluate Model Performance (Last 24 Hours Only) ===
print("\n=== Model Performance (Last 24 Hours of 48-Hour Forecasts) ===")

test_mape_historical = np.mean(np.abs((test_actual_final - test_forecasts_final) / test_actual_final)) * 100
test_mae_historical = np.mean(np.abs(test_actual_final - test_forecasts_final))
test_rmse_historical = np.sqrt(np.mean((test_actual_final - test_forecasts_final) ** 2))

print(f"Test MAPE (Historical Forecasts, Last 24h): {test_mape_historical:.2f}%")
print(f"Test MAE (Historical Forecasts, Last 24h): {test_mae_historical:.2f}")
print(f"Test RMSE (Historical Forecasts, Last 24h): {test_rmse_historical:.2f}")

# === Create Plotly Visualization ===
print("\nCreating Plotly visualization...")

# Create DataFrame for plotting
plot_data = pd.DataFrame({
    'UTC_time': time_final,
    'Actual': test_actual_final,
    'Forecast': test_forecasts_final
})

# Calculate error for plotting
plot_data['Error'] = plot_data['Forecast'] - plot_data['Actual']

# Simple comparison plot
fig1 = px.line(plot_data, x='UTC_time', y=['Actual', 'Forecast'],
               title='Actual vs Forecasted Demand (Last 24h of 48h Historical Forecasts)',
               labels={'value': 'Demand (MW)', 'variable': 'Type'})

fig1.update_layout(
    xaxis_title='UTC Time',
    yaxis_title='Demand (MW)',
    hovermode='x unified',
    width=1200,
    height=600
)

fig1.show()

# === Print Summary Statistics ===
print("\n=== Summary Statistics ===")
print(f"Mean Actual Demand: {plot_data['Actual'].mean():.2f} MW")
print(f"Mean Forecasted Demand: {plot_data['Forecast'].mean():.2f} MW")
print(f"Mean Absolute Error: {abs(plot_data['Error']).mean():.2f} MW")
print(f"Root Mean Square Error: {(plot_data['Error']**2).mean()**0.5:.2f} MW")
print(f"Forecast period: {plot_data['UTC_time'].min()} to {plot_data['UTC_time'].max()}")
print(f"Number of forecast points: {len(plot_data)}")
print(f"Data coverage: {len(plot_data)/24:.1f} days")

print("\nProcess completed successfully!")
print("- Data preprocessed and missing values filled")
print("- Model trained with custom loss function focusing on last 24 hours")
print("- Historical forecasts generated and evaluated")
print("- Visualizations created")

"""## Not working

### Create merged timeseries
"""

import pandas as pd
import numpy as np

def process_excel_file(file_path):
    """Process Excel file with multiple sheets"""
    print(f"Processing {file_path}...")
    all_data = []
    xls = pd.ExcelFile(file_path)

    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name)
        time_slots = df.iloc[:, 0]
        date_range = df.columns[1:]

        for i, date in enumerate(date_range):
            col_data = df.iloc[:, i + 1]
            full_datetimes = [pd.to_datetime(f"{date} {ts.split(' - ')[0]}") for ts in time_slots]

            temp_df = pd.DataFrame({
                'ds': full_datetimes,
                'y': col_data,
                'unique_id': 'delhi_demand'
            })
            all_data.append(temp_df)

    result = pd.concat(all_data, ignore_index=True)
    result = result.dropna().drop_duplicates(subset=['ds']).sort_values('ds')
    print(f"Extracted {len(result)} records")
    return result

def process_brpl_txt(file_path):
    """Process BRPL text file"""
    print(f"Processing {file_path}...")
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()
    df['Data_Time'] = df['Data_Time'].str.strip()
    df['ds'] = pd.to_datetime(df['Data_Time'], format='%d-%b-%Y %I:%M:%S %p')
    df['y'] = df['Load']
    df['unique_id'] = 'delhi_demand'
    result = df[['unique_id', 'ds', 'y']].dropna().sort_values('ds')
    print(f"Extracted {len(result)} records")
    return result

def interpolate_15min(df):
    """Interpolate to 15-minute frequency"""
    if len(df) == 0:
        return df

    # Create 15-minute range
    full_range = pd.date_range(start=df['ds'].min(), end=df['ds'].max(), freq='15T')
    full_df = pd.DataFrame({'ds': full_range, 'unique_id': 'delhi_demand'})

    # Merge and interpolate
    merged = pd.merge(full_df, df[['ds', 'y']], on='ds', how='left')
    merged['y'] = merged['y'].interpolate(method='linear').bfill().ffill()

    return merged[['unique_id', 'ds', 'y']]

# Main execution
print("Loading original timeseries...")
timeseries = pd.read_csv('/content/timeseries.csv')
timeseries['ds'] = pd.to_datetime(timeseries['ds'])

print("Processing April-May Excel...")
april_data = process_excel_file('/content/BRPL Demand 1st April-31st May.xlsx')

print("Processing June onwards Excel...")
june_data = process_excel_file('/content/BRPL Demand 1st June onwards.xlsx')

print("Processing BRPL text data...")
brpl_data = process_brpl_txt('/content/brpl_data.txt')

# Define cutoff points
timeseries_end = pd.Timestamp("2025-03-31 23:45:00")
excel_cutoff = pd.Timestamp("2025-07-01 10:00:00")

print("Combining data...")

# Start with original timeseries
combined = timeseries.copy()

# Add Excel data (April 1 to July 1 10:00 AM)
excel_combined = pd.concat([april_data, june_data]).drop_duplicates(subset=['ds']).sort_values('ds')
excel_filtered = excel_combined[(excel_combined['ds'] > timeseries_end) &
                               (excel_combined['ds'] <= excel_cutoff)]

if len(excel_filtered) > 0:
    excel_interpolated = interpolate_15min(excel_filtered)
    combined = pd.concat([combined, excel_interpolated])

# Add BRPL data (after July 1 10:00 AM)
brpl_filtered = brpl_data[brpl_data['ds'] > excel_cutoff]

if len(brpl_filtered) > 0:
    brpl_interpolated = interpolate_15min(brpl_filtered)
    combined = pd.concat([combined, brpl_interpolated])

# Final cleanup
combined = combined.drop_duplicates(subset=['ds']).sort_values('ds').reset_index(drop=True)
combined['y'] = combined['y'].interpolate(method='linear').bfill().ffill()

print(f"\nFinal result:")
print(f"Records: {len(combined)}")
print(f"Range: {combined['ds'].min()} to {combined['ds'].max()}")
print(f"Missing values: {combined['y'].isna().sum()}")

# Save result
combined.to_csv('/content/merged_timeseries.csv', index=False)
print("Saved to /content/merged_timeseries.csv")

# === DEPLOYMENT SCRIPT FOR TIDE MODEL FORECASTING ===
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from meteostat import Hourly
from darts import TimeSeries
from darts.models import TiDEModel
import torch
import holidays
import pickle
import warnings
import openpyxl

warnings.filterwarnings("ignore")

print("=" * 60)
print("STARTING DEPLOYMENT FOR ELECTRICITY DEMAND FORECASTING")
print("=" * 60)

# === Calendar Features Function ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add 15-minute specific features
    features["quarter_hour_sin"] = np.sin(2 * np.pi * index.minute / 60)
    features["quarter_hour_cos"] = np.cos(2 * np.pi * index.minute / 60)

    in_holidays = holidays.country_holidays("IN", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(in_holidays.keys()))).astype(int)
    return features

print("\n=== LOADING TRAINED MODEL AND SCALERS ===")

# Load the trained model
print("Loading trained model...")
model = TiDEModel.load("/content/final_tide_model.pkl")
print("Model loaded successfully")

# Load scalers
print("Loading scalers...")
with open("/content/final_scaler_y.pkl", "rb") as f:
    scaler_y = pickle.load(f)

with open("/content/final_scaler_future_cov.pkl", "rb") as f:
    scaler_future_cov = pickle.load(f)

print("Scalers loaded successfully")

print("\n=== LOADING UPDATED DEMAND DATA ===")

# Load updated demand data
df = pd.read_csv("merged_timeseries.csv")
df.columns = df.columns.str.strip()
df["ds"] = pd.to_datetime(df["ds"])
df.set_index("ds", inplace=True)

# Drop unique_id if it exists
if "unique_id" in df.columns:
    df = df.drop("unique_id", axis=1)

df = df.sort_index()

print("Updated demand data loaded")
print(f"  Shape: {df.shape}")
print(f"  Range: {df.index.min()} to {df.index.max()}")

# Get the last timestamp
last_timestamp = df.index.max()
forecast_start = last_timestamp
print(f"  Last timestamp (forecast start): {forecast_start}")

print("\n=== SETTING UP FORECAST PARAMETERS ===")

# Model parameters
input_length = 124  # timesteps (31 hours)
forecast_length = 164  # timesteps (41 hours)

# Calculate required data range
# We need 124 timesteps BEFORE the forecast start
input_start = forecast_start - pd.Timedelta(minutes=(input_length - 1) * 15)
forecast_end = forecast_start + pd.Timedelta(minutes=forecast_length * 15)

print("Forecast parameters:")
print(f"  Input data needed from: {input_start}")
print(f"  Forecast start: {forecast_start}")
print(f"  Forecast end: {forecast_end}")
print(f"  Input length: {input_length} timesteps ({input_length * 15 / 60:.1f} hours)")
print(f"  Forecast length: {forecast_length} timesteps ({forecast_length * 15 / 60:.1f} hours)")

print("\n=== CREATING EXTENDED TIME INDEX ===")

# Create extended index for features (input period + forecast period + buffer)
feature_end = forecast_end + pd.Timedelta(hours=6)  # Extra buffer
extended_index = pd.date_range(
    start=input_start,
    end=feature_end,
    freq='15min'
)

print("Extended index created:")
print(f"  Range: {extended_index.min()} to {extended_index.max()}")
print(f"  Length: {len(extended_index)} timesteps")

print("\n=== GENERATING CALENDAR FEATURES ===")

calendar_features = create_calendar_features(extended_index)
print(f"Calendar features generated: {calendar_features.shape}")

print("\n=== FETCHING WEATHER DATA ===")

# Use the same station ID as in training
station_ids = ['42182']
weather_stations = {}

for station_id in station_ids:
    try:
        print(f"  Fetching weather data for station: {station_id}")

        # Fetch weather data with buffer
        weather_start = input_start - pd.Timedelta(hours=12)
        weather_end = feature_end

        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres"]]

        print(f"    Raw weather data shape: {weather_data.shape}")

        # Fill missing values at hourly level
        weather_data = weather_data.interpolate(method='linear', limit_direction='both')

        # Reindex to 15-minute frequency and interpolate
        weather_15min = weather_data.reindex(extended_index)
        weather_15min = weather_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values
        weather_15min = weather_15min.bfill().ffill()

        # Rename columns to match training format
        weather_15min.columns = [f"station_{station_id}_{col}" for col in weather_15min.columns]

        weather_stations[station_id] = weather_15min

        print(f"    Station {station_id}: {weather_15min.shape[1]} features, {weather_15min.isnull().sum().sum()} missing values")

    except Exception as e:
        print(f"    Error loading station {station_id}: {e}")

# Combine weather data
weather_df = pd.concat(list(weather_stations.values()), axis=1)
print(f"Total weather features: {weather_df.shape[1]}")

print("\n=== CREATING FUTURE COVARIATES ===")

# Combine calendar and weather features
future_features = pd.concat([calendar_features, weather_df], axis=1)

# Add the same feature engineering as in training
# Add squared terms for temperature variables
weather_cols = [col for col in weather_df.columns if 'temp' in col]
for col in weather_cols:
    future_features[f"{col}_squared"] = weather_df[col] ** 2

print("Future covariates created:")
print(f"  Total features: {future_features.shape[1]}")
print(f"  Calendar features: {calendar_features.shape[1]}")
print(f"  Weather features: {weather_df.shape[1]}")
print(f"  Engineered features: {future_features.shape[1] - calendar_features.shape[1] - weather_df.shape[1]}")

print("\n=== PREPARING DATA FOR PREDICTION ===")

# Create the input target series (historical data up to forecast start)
input_data = df.loc[input_start:forecast_start].copy()

# Check if we have enough input data
if len(input_data) < input_length:
    print(f"Error: Need {input_length} timesteps, but only have {len(input_data)}")
    print(f"  Available data range: {input_data.index.min()} to {input_data.index.max()}")
    raise ValueError("Insufficient historical data for prediction")

print(f"Input data prepared:")
print(f"  Shape: {input_data.shape}")
print(f"  Range: {input_data.index.min()} to {input_data.index.max()}")

# Create TimeSeries objects
target_ts = TimeSeries.from_dataframe(input_data, value_cols=["y"])
future_covariates_ts = TimeSeries.from_dataframe(future_features)

print(f"TimeSeries objects created:")
print(f"  Target: {target_ts.start_time()} to {target_ts.end_time()} ({len(target_ts)} timesteps)")
print(f"  Future covariates: {future_covariates_ts.start_time()} to {future_covariates_ts.end_time()} ({len(future_covariates_ts)} timesteps)")

print("\n=== SCALING DATA ===")

# Scale the input data using the fitted scalers
scaled_target = scaler_y.transform(target_ts).astype(np.float32)
scaled_future_cov = scaler_future_cov.transform(future_covariates_ts).astype(np.float32)

print("Data scaled successfully")

print("\n=== MAKING FORECAST ===")

try:
    print(f"Making {forecast_length}-step forecast...")

    # Make prediction
    forecast_scaled = model.predict(
        n=forecast_length,
        series=scaled_target,
        future_covariates=scaled_future_cov
    )

    # Inverse transform the forecast
    forecast = scaler_y.inverse_transform(forecast_scaled)

    print("✓ Forecast completed successfully!")

    # Convert to DataFrame for easier handling
    forecast_df = forecast.to_dataframe()
    forecast_df.columns = ['demand_forecast']

    print(f"✓ Forecast range: {forecast_df.index.min()} to {forecast_df.index.max()}")
    print(f"✓ Forecast length: {len(forecast_df)} timesteps")

except Exception as e:
    print(f"✗ Error during forecasting: {e}")
    raise

print("\n=== EXTRACTING LAST 24 HOURS ===")

# Extract only the last 24 hours (96 timesteps for 15-minute data)
last_24h_timesteps = 96
forecast_last_24h = forecast_df.tail(last_24h_timesteps)

print(f"Last 24 hours extracted:")
print(f"  Period: {forecast_last_24h.index.min()} to {forecast_last_24h.index.max()}")
print(f"  Timesteps: {len(forecast_last_24h)}")

print("\n=== FORMATTING FOR EXCEL OUTPUT ===")

# Create time slot labels for 15-minute intervals
time_slots = []
for hour in range(24):
    for minute in [0, 15, 30, 45]:
        start_time = f"{hour:02d}:{minute:02d}"
        if minute == 45:
            end_hour = (hour + 1) % 24
            end_time = f"{end_hour:02d}:00"
            if end_hour == 0:
                end_time = "24:00"
        else:
            end_time = f"{hour:02d}:{minute + 15:02d}"
        time_slots.append(f"{start_time} - {end_time}")

print(f"Created {len(time_slots)} time slot labels")

# Get the date for the forecast (using the first date in the 24-hour period)
forecast_date = forecast_last_24h.index[0].strftime("%-m/%-d/%y")  # Format: 3/17/24

# Create the output DataFrame
output_df = pd.DataFrame({
    'Time slot': time_slots,
    forecast_date: forecast_last_24h['demand_forecast'].values
})

print(f"Excel format created for date: {forecast_date}")

print("\n=== SAVING RESULTS ===")

# Save to Excel format
output_df.to_excel("electricity_demand_forecast_24h.xlsx", index=False)
print("Last 24-hour forecast saved to 'electricity_demand_forecast_24h.xlsx'")

# Also save CSV for backup
forecast_last_24h.to_csv("electricity_demand_forecast_24h.csv")
print("Backup CSV saved to 'electricity_demand_forecast_24h.csv'")

print("\n=== LAST 24 HOUR FORECAST PREVIEW ===")
print("Excel format preview:")
print(output_df.head(10))
print("...")
print(output_df.tail(10))

print("\n" + "=" * 60)
print("DEPLOYMENT COMPLETED SUCCESSFULLY!")
print("=" * 60)
print(f"Excel forecast file: electricity_demand_forecast_24h.xlsx")
print(f"Backup CSV file: electricity_demand_forecast_24h.csv")
print(f"Forecast date: {forecast_date}")
print(f"Total timesteps: {len(forecast_last_24h)} (24 hours)")

"""## Archives"""

# === DEPLOYMENT SCRIPT FOR TIDE MODEL FORECASTING ===
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from meteostat import Hourly
from darts import TimeSeries
from darts.models import TiDEModel
import torch
import holidays
import pickle
import warnings

warnings.filterwarnings("ignore")

print("=" * 60)
print("STARTING DEPLOYMENT FOR ELECTRICITY DEMAND FORECASTING")
print("=" * 60)

# === Calendar Features Function ===
def create_calendar_features(index):
    features = pd.DataFrame(index=index)
    features["hour_sin"] = np.sin(2 * np.pi * index.hour / 24)
    features["hour_cos"] = np.cos(2 * np.pi * index.hour / 24)
    features["dow_sin"] = np.sin(2 * np.pi * index.dayofweek / 7)
    features["dow_cos"] = np.cos(2 * np.pi * index.dayofweek / 7)
    features["month_sin"] = np.sin(2 * np.pi * index.month / 12)
    features["month_cos"] = np.cos(2 * np.pi * index.month / 12)
    features["is_weekend"] = (index.dayofweek >= 5).astype(int)

    # Add 15-minute specific features
    features["quarter_hour_sin"] = np.sin(2 * np.pi * index.minute / 60)
    features["quarter_hour_cos"] = np.cos(2 * np.pi * index.minute / 60)

    in_holidays = holidays.country_holidays("IN", years=range(index.year.min(), index.year.max() + 1))
    features["is_holiday"] = index.normalize().isin(pd.to_datetime(list(in_holidays.keys()))).astype(int)
    return features

print("\n=== LOADING TRAINED MODEL AND SCALERS ===")

# Load the trained model
print("Loading trained model...")
model = TiDEModel.load("/content/final_tide_model.pkl")
print("✓ Model loaded successfully")

# Load scalers
print("Loading scalers...")
with open("/content/final_scaler_y.pkl", "rb") as f:
    scaler_y = pickle.load(f)

with open("/content/final_scaler_future_cov.pkl", "rb") as f:
    scaler_future_cov = pickle.load(f)

print("✓ Scalers loaded successfully")

print("\n=== LOADING UPDATED DEMAND DATA ===")

# Load updated demand data
df = pd.read_csv("merged_timeseries.csv")
df.columns = df.columns.str.strip()
df["ds"] = pd.to_datetime(df["ds"])
df.set_index("ds", inplace=True)

# Drop unique_id if it exists
if "unique_id" in df.columns:
    df = df.drop("unique_id", axis=1)

df = df.sort_index()

print(f"✓ Updated demand data loaded")
print(f"  Shape: {df.shape}")
print(f"  Range: {df.index.min()} to {df.index.max()}")

# Get the last timestamp
last_timestamp = df.index.max()
forecast_start = last_timestamp
print(f"  Last timestamp (forecast start): {forecast_start}")

print("\n=== SETTING UP FORECAST PARAMETERS ===")

# Model parameters
input_length = 124  # timesteps (31 hours)
forecast_length = 164  # timesteps (41 hours)

# Calculate required data range
# We need 124 timesteps BEFORE the forecast start
input_start = forecast_start - pd.Timedelta(minutes=(input_length - 1) * 15)
forecast_end = forecast_start + pd.Timedelta(minutes=forecast_length * 15)

print(f"✓ Forecast parameters:")
print(f"  Input data needed from: {input_start}")
print(f"  Forecast start: {forecast_start}")
print(f"  Forecast end: {forecast_end}")
print(f"  Input length: {input_length} timesteps ({input_length * 15 / 60:.1f} hours)")
print(f"  Forecast length: {forecast_length} timesteps ({forecast_length * 15 / 60:.1f} hours)")

print("\n=== CREATING EXTENDED TIME INDEX ===")

# Create extended index for features (input period + forecast period + buffer)
feature_end = forecast_end + pd.Timedelta(hours=6)  # Extra buffer
extended_index = pd.date_range(
    start=input_start,
    end=feature_end,
    freq='15min'
)

print(f"✓ Extended index created:")
print(f"  Range: {extended_index.min()} to {extended_index.max()}")
print(f"  Length: {len(extended_index)} timesteps")

print("\n=== GENERATING CALENDAR FEATURES ===")

calendar_features = create_calendar_features(extended_index)
print(f"✓ Calendar features generated: {calendar_features.shape}")

print("\n=== FETCHING WEATHER DATA ===")

# Use the same station ID as in training
station_ids = ['42182']
weather_stations = {}

for station_id in station_ids:
    try:
        print(f"  Fetching weather data for station: {station_id}")

        # Fetch weather data with buffer
        weather_start = input_start - pd.Timedelta(hours=12)
        weather_end = feature_end

        weather_data = Hourly(station_id, weather_start, weather_end).fetch()
        weather_data = weather_data[["temp", "dwpt", "rhum", "wspd", "pres"]]

        print(f"    Raw weather data shape: {weather_data.shape}")

        # Fill missing values at hourly level
        weather_data = weather_data.interpolate(method='linear', limit_direction='both')

        # Reindex to 15-minute frequency and interpolate
        weather_15min = weather_data.reindex(extended_index)
        weather_15min = weather_15min.interpolate(method='linear', limit_direction='both')

        # Handle any remaining NaN values
        weather_15min = weather_15min.bfill().ffill()

        # Rename columns to match training format
        weather_15min.columns = [f"station_{station_id}_{col}" for col in weather_15min.columns]

        weather_stations[station_id] = weather_15min

        print(f"    ✓ Station {station_id}: {weather_15min.shape[1]} features, {weather_15min.isnull().sum().sum()} missing values")

    except Exception as e:
        print(f"    ✗ Error loading station {station_id}: {e}")

# Combine weather data
weather_df = pd.concat(list(weather_stations.values()), axis=1)
print(f"✓ Total weather features: {weather_df.shape[1]}")

print("\n=== CREATING FUTURE COVARIATES ===")

# Combine calendar and weather features
future_features = pd.concat([calendar_features, weather_df], axis=1)

# Add the same feature engineering as in training
# Add squared terms for temperature variables
weather_cols = [col for col in weather_df.columns if 'temp' in col]
for col in weather_cols:
    future_features[f"{col}_squared"] = weather_df[col] ** 2

print(f"✓ Future covariates created:")
print(f"  Total features: {future_features.shape[1]}")
print(f"  Calendar features: {calendar_features.shape[1]}")
print(f"  Weather features: {weather_df.shape[1]}")
print(f"  Engineered features: {future_features.shape[1] - calendar_features.shape[1] - weather_df.shape[1]}")

print("\n=== PREPARING DATA FOR PREDICTION ===")

# Create the input target series (historical data up to forecast start)
input_data = df.loc[input_start:forecast_start].copy()

# Check if we have enough input data
if len(input_data) < input_length:
    print(f"✗ Error: Need {input_length} timesteps, but only have {len(input_data)}")
    print(f"  Available data range: {input_data.index.min()} to {input_data.index.max()}")
    raise ValueError("Insufficient historical data for prediction")

print(f"✓ Input data prepared:")
print(f"  Shape: {input_data.shape}")
print(f"  Range: {input_data.index.min()} to {input_data.index.max()}")

# Create TimeSeries objects
target_ts = TimeSeries.from_dataframe(input_data, value_cols=["y"])
future_covariates_ts = TimeSeries.from_dataframe(future_features)

print(f"✓ TimeSeries objects created:")
print(f"  Target: {target_ts.start_time()} to {target_ts.end_time()} ({len(target_ts)} timesteps)")
print(f"  Future covariates: {future_covariates_ts.start_time()} to {future_covariates_ts.end_time()} ({len(future_covariates_ts)} timesteps)")

print("\n=== SCALING DATA ===")

# Scale the input data using the fitted scalers
scaled_target = scaler_y.transform(target_ts).astype(np.float32)
scaled_future_cov = scaler_future_cov.transform(future_covariates_ts).astype(np.float32)

print("✓ Data scaled successfully")

print("\n=== MAKING FORECAST ===")

try:
    print(f"Making {forecast_length}-step forecast...")

    # Make prediction
    forecast_scaled = model.predict(
        n=forecast_length,
        series=scaled_target,
        future_covariates=scaled_future_cov
    )

    # Inverse transform the forecast
    forecast = scaler_y.inverse_transform(forecast_scaled)

    print("✓ Forecast completed successfully!")

    # Convert to DataFrame for easier handling
    forecast_df = forecast.to_dataframe()
    forecast_df.columns = ['demand_forecast']

    print(f"✓ Forecast range: {forecast_df.index.min()} to {forecast_df.index.max()}")
    print(f"✓ Forecast length: {len(forecast_df)} timesteps")

except Exception as e:
    print(f"✗ Error during forecasting: {e}")
    raise

print("\n=== SAVING RESULTS ===")

# Save forecast results
forecast_df.to_csv("electricity_demand_forecast.csv")
print("✓ Forecast saved to 'electricity_demand_forecast.csv'")

# Create summary statistics
summary_stats = {
    'forecast_start': forecast_start,
    'forecast_end': forecast_df.index.max(),
    'num_timesteps': len(forecast_df),
    'min_demand': forecast_df['demand_forecast'].min(),
    'max_demand': forecast_df['demand_forecast'].max(),
    'mean_demand': forecast_df['demand_forecast'].mean(),
    'std_demand': forecast_df['demand_forecast'].std()
}

print("\n=== FORECAST SUMMARY ===")
for key, value in summary_stats.items():
    if isinstance(value, float):
        print(f"{key}: {value:.2f}")
    else:
        print(f"{key}: {value}")

print("\n=== SAMPLE FORECAST VALUES ===")
print("First 10 predictions:")
print(forecast_df.head(10))
print("\nLast 10 predictions:")
print(forecast_df.tail(10))

print("\n" + "=" * 60)
print("DEPLOYMENT COMPLETED SUCCESSFULLY!")
print("=" * 60)
print(f"Forecast file: electricity_demand_forecast.csv")
print(f"Forecast period: {forecast_start} to {forecast_df.index.max()}")
print(f"Total timesteps: {len(forecast_df)} (≈{len(forecast_df) * 15 / 60:.1f} hours)")

forecast_df[-96:]

